{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Homework solution PyTorch Diabetes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD_RSER5Lkb5"
      },
      "source": [
        "# Homework PyTorch Diabetes\n",
        "\n",
        "https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\n",
        "\n",
        "Homework instruction: diabetes classification:\n",
        "\n",
        "Here is the Practical Activity regarding the PyTorch library:\n",
        "Use the diabetes.csv attached to this class in order to classify whether or not a person has diabetes. You can see here more information about the dataset.\n",
        "\n",
        "Hints:\n",
        "\n",
        "•\tUpload the .csv file into the Google Colab environment\n",
        "\n",
        "•\tUse the pandas library in order to load the csv file - check the read_csv function\n",
        "\n",
        "•\tIt's a binary classification problem, class 0 (no disease) and class 1 (disease)\n",
        "\n",
        "•\tYou need to normalize the data, check the MinMaxScaler from sklearn.\n",
        "\n",
        "https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tP2BcEILoLB"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U7Zv8382lu4",
        "outputId": "83305526-617d-476a-fcc0-f6cbee13f275"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (703.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 703.8 MB 22 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu101) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0+cu101) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.5.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.5.0+cu101 torchvision-0.6.0+cu101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b85d_o8BdFub",
        "outputId": "3731f4ba-5fc2-4ed5-ce56-f69b0ed16abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.0+cu101'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1WlABbCcw2B"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch.nn as nn"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFSnAUTVQN2j"
      },
      "source": [
        "diabetes = pd.read_csv('diabetes.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIoAdvbMuaUD",
        "outputId": "04ee043d-82b1-40b7-dd8e-eb39e1acdaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "diabetes.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aab38142-32e7-44ff-aeae-5aa633b6d255\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aab38142-32e7-44ff-aeae-5aa633b6d255')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aab38142-32e7-44ff-aeae-5aa633b6d255 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aab38142-32e7-44ff-aeae-5aa633b6d255');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2L0Zo3oQg66",
        "outputId": "5d7961ff-7c71-4e4e-ca6a-81075c787d8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inputs = diabetes.iloc[:, 0:8].values\n",
        "inputs.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw6crTDBfaLl",
        "outputId": "320b42ef-38bf-4e93-f51d-5536211856b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inputs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  6.   , 148.   ,  72.   , ...,  33.6  ,   0.627,  50.   ],\n",
              "       [  1.   ,  85.   ,  66.   , ...,  26.6  ,   0.351,  31.   ],\n",
              "       [  8.   , 183.   ,  64.   , ...,  23.3  ,   0.672,  32.   ],\n",
              "       ...,\n",
              "       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ],\n",
              "       [  1.   , 126.   ,  60.   , ...,  30.1  ,   0.349,  47.   ],\n",
              "       [  1.   ,  93.   ,  70.   , ...,  30.4  ,   0.315,  23.   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKVjNkwkumY7"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "inputs = scaler.fit_transform(inputs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk9mvY2AuvJq",
        "outputId": "efb9e6c9-e984-4eb3-acbe-fe0710dd13fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inputs"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35294118, 0.74371859, 0.59016393, ..., 0.50074516, 0.23441503,\n",
              "        0.48333333],\n",
              "       [0.05882353, 0.42713568, 0.54098361, ..., 0.39642325, 0.11656704,\n",
              "        0.16666667],\n",
              "       [0.47058824, 0.91959799, 0.52459016, ..., 0.34724292, 0.25362938,\n",
              "        0.18333333],\n",
              "       ...,\n",
              "       [0.29411765, 0.6080402 , 0.59016393, ..., 0.390462  , 0.07130658,\n",
              "        0.15      ],\n",
              "       [0.05882353, 0.63316583, 0.49180328, ..., 0.4485842 , 0.11571307,\n",
              "        0.43333333],\n",
              "       [0.05882353, 0.46733668, 0.57377049, ..., 0.45305514, 0.10119556,\n",
              "        0.03333333]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwoclkDjQq-6",
        "outputId": "7c8f4991-e810-4e4b-ad71-2705c26afb2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "outputs = diabetes.iloc[:,8].values\n",
        "outputs"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX9ZDhUMgBsv"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size = 0.20)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SEVQyX9gR4O",
        "outputId": "b5bd34a2-2257-461a-d90e-62c31b3dfc43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(614, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKyN_plmgZzH",
        "outputId": "3a1738d0-b056-4025-e90c-4231ead97f09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(154, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72uvlxJrOuWd"
      },
      "source": [
        "## Data transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk5Gjgb7hBCo",
        "outputId": "fc8ac20f-9b27-41a3-abec-342a14d4f8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqPER9AYhTpt"
      },
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float)\n",
        "y_train = torch.tensor(y_train, dtype = torch.float)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2HrvEJh5Km",
        "outputId": "f0c30f97-a13a-4c1c-ff33-ae78ce9b7216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__6a-iZhiJVI"
      },
      "source": [
        "dataset = torch.utils.data.TensorDataset(X_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0sP_kvViZJl",
        "outputId": "a1c409e0-eec4-4d02-bccc-0f350f0f656b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.TensorDataset"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dIWzA4wihKD"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=10)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGDLesyDQpIb"
      },
      "source": [
        "## Neural network structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaSPjUFhatlQ",
        "outputId": "640197a4-bd5c-4ce9-f8a8-05cd180f3300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(8 + 1) / 2"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FAFgY56jmdG"
      },
      "source": [
        "# 8 -> 5 -> 5 -> 1\n",
        "network = nn.Sequential(nn.Linear(8, 5),\n",
        "                          nn.Sigmoid(),\n",
        "                          nn.Linear(5, 5),\n",
        "                          nn.Sigmoid(),\n",
        "                          nn.Linear(5, 1),\n",
        "                          nn.Sigmoid())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixwI_mhZlDVW",
        "outputId": "58c5524b-57ea-4fc6-d495-35b4b64f02db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "network.parameters"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (0): Linear(in_features=8, out_features=5, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (3): Sigmoid()\n",
              "  (4): Linear(in_features=5, out_features=1, bias=True)\n",
              "  (5): Sigmoid()\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--bNcvlplMh9"
      },
      "source": [
        "loss_function = nn.BCELoss()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um6Tr3s_lXHK"
      },
      "source": [
        "optimizer = torch.optim.Adam(network.parameters(), lr=0.01)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exieZFSam_eI",
        "outputId": "bea258b5-3da1-44a2-959a-06135986a0c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epochs = 2000\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.\n",
        "\n",
        "  for data in train_loader:\n",
        "    inputs, outputs = data\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = network.forward(inputs)\n",
        "    loss = loss_function(predictions, outputs)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "  print('Epoch %3d: loss %.5f' % (epoch+1, running_loss/len(train_loader)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1: loss 0.66184\n",
            "Epoch   2: loss 0.65127\n",
            "Epoch   3: loss 0.64998\n",
            "Epoch   4: loss 0.64729\n",
            "Epoch   5: loss 0.64244\n",
            "Epoch   6: loss 0.63367\n",
            "Epoch   7: loss 0.61938\n",
            "Epoch   8: loss 0.60091\n",
            "Epoch   9: loss 0.58244\n",
            "Epoch  10: loss 0.56705\n",
            "Epoch  11: loss 0.55498\n",
            "Epoch  12: loss 0.54535\n",
            "Epoch  13: loss 0.53731\n",
            "Epoch  14: loss 0.53029\n",
            "Epoch  15: loss 0.52391\n",
            "Epoch  16: loss 0.51793\n",
            "Epoch  17: loss 0.51224\n",
            "Epoch  18: loss 0.50683\n",
            "Epoch  19: loss 0.50178\n",
            "Epoch  20: loss 0.49719\n",
            "Epoch  21: loss 0.49315\n",
            "Epoch  22: loss 0.48968\n",
            "Epoch  23: loss 0.48673\n",
            "Epoch  24: loss 0.48426\n",
            "Epoch  25: loss 0.48219\n",
            "Epoch  26: loss 0.48043\n",
            "Epoch  27: loss 0.47894\n",
            "Epoch  28: loss 0.47766\n",
            "Epoch  29: loss 0.47653\n",
            "Epoch  30: loss 0.47553\n",
            "Epoch  31: loss 0.47462\n",
            "Epoch  32: loss 0.47379\n",
            "Epoch  33: loss 0.47301\n",
            "Epoch  34: loss 0.47229\n",
            "Epoch  35: loss 0.47161\n",
            "Epoch  36: loss 0.47096\n",
            "Epoch  37: loss 0.47034\n",
            "Epoch  38: loss 0.46975\n",
            "Epoch  39: loss 0.46918\n",
            "Epoch  40: loss 0.46865\n",
            "Epoch  41: loss 0.46813\n",
            "Epoch  42: loss 0.46764\n",
            "Epoch  43: loss 0.46717\n",
            "Epoch  44: loss 0.46672\n",
            "Epoch  45: loss 0.46630\n",
            "Epoch  46: loss 0.46589\n",
            "Epoch  47: loss 0.46551\n",
            "Epoch  48: loss 0.46514\n",
            "Epoch  49: loss 0.46479\n",
            "Epoch  50: loss 0.46446\n",
            "Epoch  51: loss 0.46415\n",
            "Epoch  52: loss 0.46385\n",
            "Epoch  53: loss 0.46357\n",
            "Epoch  54: loss 0.46330\n",
            "Epoch  55: loss 0.46305\n",
            "Epoch  56: loss 0.46281\n",
            "Epoch  57: loss 0.46258\n",
            "Epoch  58: loss 0.46236\n",
            "Epoch  59: loss 0.46215\n",
            "Epoch  60: loss 0.46195\n",
            "Epoch  61: loss 0.46176\n",
            "Epoch  62: loss 0.46159\n",
            "Epoch  63: loss 0.46142\n",
            "Epoch  64: loss 0.46125\n",
            "Epoch  65: loss 0.46110\n",
            "Epoch  66: loss 0.46095\n",
            "Epoch  67: loss 0.46081\n",
            "Epoch  68: loss 0.46068\n",
            "Epoch  69: loss 0.46055\n",
            "Epoch  70: loss 0.46043\n",
            "Epoch  71: loss 0.46031\n",
            "Epoch  72: loss 0.46020\n",
            "Epoch  73: loss 0.46009\n",
            "Epoch  74: loss 0.45999\n",
            "Epoch  75: loss 0.45989\n",
            "Epoch  76: loss 0.45979\n",
            "Epoch  77: loss 0.45970\n",
            "Epoch  78: loss 0.45961\n",
            "Epoch  79: loss 0.45953\n",
            "Epoch  80: loss 0.45945\n",
            "Epoch  81: loss 0.45937\n",
            "Epoch  82: loss 0.45930\n",
            "Epoch  83: loss 0.45922\n",
            "Epoch  84: loss 0.45915\n",
            "Epoch  85: loss 0.45909\n",
            "Epoch  86: loss 0.45902\n",
            "Epoch  87: loss 0.45896\n",
            "Epoch  88: loss 0.45889\n",
            "Epoch  89: loss 0.45884\n",
            "Epoch  90: loss 0.45878\n",
            "Epoch  91: loss 0.45872\n",
            "Epoch  92: loss 0.45867\n",
            "Epoch  93: loss 0.45861\n",
            "Epoch  94: loss 0.45856\n",
            "Epoch  95: loss 0.45851\n",
            "Epoch  96: loss 0.45846\n",
            "Epoch  97: loss 0.45842\n",
            "Epoch  98: loss 0.45837\n",
            "Epoch  99: loss 0.45832\n",
            "Epoch 100: loss 0.45828\n",
            "Epoch 101: loss 0.45823\n",
            "Epoch 102: loss 0.45819\n",
            "Epoch 103: loss 0.45815\n",
            "Epoch 104: loss 0.45811\n",
            "Epoch 105: loss 0.45807\n",
            "Epoch 106: loss 0.45803\n",
            "Epoch 107: loss 0.45799\n",
            "Epoch 108: loss 0.45795\n",
            "Epoch 109: loss 0.45791\n",
            "Epoch 110: loss 0.45787\n",
            "Epoch 111: loss 0.45783\n",
            "Epoch 112: loss 0.45779\n",
            "Epoch 113: loss 0.45776\n",
            "Epoch 114: loss 0.45772\n",
            "Epoch 115: loss 0.45768\n",
            "Epoch 116: loss 0.45764\n",
            "Epoch 117: loss 0.45761\n",
            "Epoch 118: loss 0.45757\n",
            "Epoch 119: loss 0.45753\n",
            "Epoch 120: loss 0.45749\n",
            "Epoch 121: loss 0.45746\n",
            "Epoch 122: loss 0.45742\n",
            "Epoch 123: loss 0.45738\n",
            "Epoch 124: loss 0.45734\n",
            "Epoch 125: loss 0.45731\n",
            "Epoch 126: loss 0.45727\n",
            "Epoch 127: loss 0.45723\n",
            "Epoch 128: loss 0.45719\n",
            "Epoch 129: loss 0.45715\n",
            "Epoch 130: loss 0.45711\n",
            "Epoch 131: loss 0.45707\n",
            "Epoch 132: loss 0.45703\n",
            "Epoch 133: loss 0.45699\n",
            "Epoch 134: loss 0.45694\n",
            "Epoch 135: loss 0.45690\n",
            "Epoch 136: loss 0.45686\n",
            "Epoch 137: loss 0.45682\n",
            "Epoch 138: loss 0.45677\n",
            "Epoch 139: loss 0.45673\n",
            "Epoch 140: loss 0.45668\n",
            "Epoch 141: loss 0.45664\n",
            "Epoch 142: loss 0.45659\n",
            "Epoch 143: loss 0.45654\n",
            "Epoch 144: loss 0.45649\n",
            "Epoch 145: loss 0.45645\n",
            "Epoch 146: loss 0.45640\n",
            "Epoch 147: loss 0.45635\n",
            "Epoch 148: loss 0.45630\n",
            "Epoch 149: loss 0.45625\n",
            "Epoch 150: loss 0.45619\n",
            "Epoch 151: loss 0.45614\n",
            "Epoch 152: loss 0.45609\n",
            "Epoch 153: loss 0.45604\n",
            "Epoch 154: loss 0.45598\n",
            "Epoch 155: loss 0.45593\n",
            "Epoch 156: loss 0.45587\n",
            "Epoch 157: loss 0.45582\n",
            "Epoch 158: loss 0.45576\n",
            "Epoch 159: loss 0.45570\n",
            "Epoch 160: loss 0.45565\n",
            "Epoch 161: loss 0.45559\n",
            "Epoch 162: loss 0.45553\n",
            "Epoch 163: loss 0.45547\n",
            "Epoch 164: loss 0.45541\n",
            "Epoch 165: loss 0.45535\n",
            "Epoch 166: loss 0.45529\n",
            "Epoch 167: loss 0.45523\n",
            "Epoch 168: loss 0.45517\n",
            "Epoch 169: loss 0.45511\n",
            "Epoch 170: loss 0.45505\n",
            "Epoch 171: loss 0.45499\n",
            "Epoch 172: loss 0.45493\n",
            "Epoch 173: loss 0.45486\n",
            "Epoch 174: loss 0.45480\n",
            "Epoch 175: loss 0.45474\n",
            "Epoch 176: loss 0.45468\n",
            "Epoch 177: loss 0.45461\n",
            "Epoch 178: loss 0.45455\n",
            "Epoch 179: loss 0.45449\n",
            "Epoch 180: loss 0.45442\n",
            "Epoch 181: loss 0.45436\n",
            "Epoch 182: loss 0.45429\n",
            "Epoch 183: loss 0.45423\n",
            "Epoch 184: loss 0.45416\n",
            "Epoch 185: loss 0.45409\n",
            "Epoch 186: loss 0.45403\n",
            "Epoch 187: loss 0.45396\n",
            "Epoch 188: loss 0.45389\n",
            "Epoch 189: loss 0.45382\n",
            "Epoch 190: loss 0.45376\n",
            "Epoch 191: loss 0.45369\n",
            "Epoch 192: loss 0.45362\n",
            "Epoch 193: loss 0.45355\n",
            "Epoch 194: loss 0.45348\n",
            "Epoch 195: loss 0.45341\n",
            "Epoch 196: loss 0.45333\n",
            "Epoch 197: loss 0.45326\n",
            "Epoch 198: loss 0.45319\n",
            "Epoch 199: loss 0.45311\n",
            "Epoch 200: loss 0.45304\n",
            "Epoch 201: loss 0.45296\n",
            "Epoch 202: loss 0.45289\n",
            "Epoch 203: loss 0.45281\n",
            "Epoch 204: loss 0.45273\n",
            "Epoch 205: loss 0.45265\n",
            "Epoch 206: loss 0.45257\n",
            "Epoch 207: loss 0.45249\n",
            "Epoch 208: loss 0.45241\n",
            "Epoch 209: loss 0.45233\n",
            "Epoch 210: loss 0.45225\n",
            "Epoch 211: loss 0.45216\n",
            "Epoch 212: loss 0.45208\n",
            "Epoch 213: loss 0.45199\n",
            "Epoch 214: loss 0.45191\n",
            "Epoch 215: loss 0.45182\n",
            "Epoch 216: loss 0.45173\n",
            "Epoch 217: loss 0.45164\n",
            "Epoch 218: loss 0.45155\n",
            "Epoch 219: loss 0.45146\n",
            "Epoch 220: loss 0.45137\n",
            "Epoch 221: loss 0.45128\n",
            "Epoch 222: loss 0.45119\n",
            "Epoch 223: loss 0.45109\n",
            "Epoch 224: loss 0.45100\n",
            "Epoch 225: loss 0.45091\n",
            "Epoch 226: loss 0.45081\n",
            "Epoch 227: loss 0.45072\n",
            "Epoch 228: loss 0.45062\n",
            "Epoch 229: loss 0.45053\n",
            "Epoch 230: loss 0.45043\n",
            "Epoch 231: loss 0.45033\n",
            "Epoch 232: loss 0.45024\n",
            "Epoch 233: loss 0.45014\n",
            "Epoch 234: loss 0.45004\n",
            "Epoch 235: loss 0.44995\n",
            "Epoch 236: loss 0.44985\n",
            "Epoch 237: loss 0.44975\n",
            "Epoch 238: loss 0.44966\n",
            "Epoch 239: loss 0.44956\n",
            "Epoch 240: loss 0.44946\n",
            "Epoch 241: loss 0.44937\n",
            "Epoch 242: loss 0.44927\n",
            "Epoch 243: loss 0.44917\n",
            "Epoch 244: loss 0.44907\n",
            "Epoch 245: loss 0.44898\n",
            "Epoch 246: loss 0.44888\n",
            "Epoch 247: loss 0.44878\n",
            "Epoch 248: loss 0.44869\n",
            "Epoch 249: loss 0.44859\n",
            "Epoch 250: loss 0.44849\n",
            "Epoch 251: loss 0.44839\n",
            "Epoch 252: loss 0.44830\n",
            "Epoch 253: loss 0.44820\n",
            "Epoch 254: loss 0.44810\n",
            "Epoch 255: loss 0.44800\n",
            "Epoch 256: loss 0.44790\n",
            "Epoch 257: loss 0.44781\n",
            "Epoch 258: loss 0.44771\n",
            "Epoch 259: loss 0.44761\n",
            "Epoch 260: loss 0.44751\n",
            "Epoch 261: loss 0.44741\n",
            "Epoch 262: loss 0.44731\n",
            "Epoch 263: loss 0.44721\n",
            "Epoch 264: loss 0.44711\n",
            "Epoch 265: loss 0.44701\n",
            "Epoch 266: loss 0.44691\n",
            "Epoch 267: loss 0.44681\n",
            "Epoch 268: loss 0.44671\n",
            "Epoch 269: loss 0.44661\n",
            "Epoch 270: loss 0.44651\n",
            "Epoch 271: loss 0.44641\n",
            "Epoch 272: loss 0.44631\n",
            "Epoch 273: loss 0.44620\n",
            "Epoch 274: loss 0.44610\n",
            "Epoch 275: loss 0.44600\n",
            "Epoch 276: loss 0.44590\n",
            "Epoch 277: loss 0.44579\n",
            "Epoch 278: loss 0.44569\n",
            "Epoch 279: loss 0.44558\n",
            "Epoch 280: loss 0.44548\n",
            "Epoch 281: loss 0.44537\n",
            "Epoch 282: loss 0.44527\n",
            "Epoch 283: loss 0.44516\n",
            "Epoch 284: loss 0.44506\n",
            "Epoch 285: loss 0.44495\n",
            "Epoch 286: loss 0.44484\n",
            "Epoch 287: loss 0.44474\n",
            "Epoch 288: loss 0.44463\n",
            "Epoch 289: loss 0.44452\n",
            "Epoch 290: loss 0.44442\n",
            "Epoch 291: loss 0.44431\n",
            "Epoch 292: loss 0.44420\n",
            "Epoch 293: loss 0.44409\n",
            "Epoch 294: loss 0.44398\n",
            "Epoch 295: loss 0.44387\n",
            "Epoch 296: loss 0.44377\n",
            "Epoch 297: loss 0.44366\n",
            "Epoch 298: loss 0.44355\n",
            "Epoch 299: loss 0.44344\n",
            "Epoch 300: loss 0.44333\n",
            "Epoch 301: loss 0.44322\n",
            "Epoch 302: loss 0.44311\n",
            "Epoch 303: loss 0.44300\n",
            "Epoch 304: loss 0.44289\n",
            "Epoch 305: loss 0.44278\n",
            "Epoch 306: loss 0.44267\n",
            "Epoch 307: loss 0.44256\n",
            "Epoch 308: loss 0.44245\n",
            "Epoch 309: loss 0.44234\n",
            "Epoch 310: loss 0.44223\n",
            "Epoch 311: loss 0.44211\n",
            "Epoch 312: loss 0.44200\n",
            "Epoch 313: loss 0.44189\n",
            "Epoch 314: loss 0.44178\n",
            "Epoch 315: loss 0.44167\n",
            "Epoch 316: loss 0.44156\n",
            "Epoch 317: loss 0.44145\n",
            "Epoch 318: loss 0.44134\n",
            "Epoch 319: loss 0.44123\n",
            "Epoch 320: loss 0.44112\n",
            "Epoch 321: loss 0.44101\n",
            "Epoch 322: loss 0.44090\n",
            "Epoch 323: loss 0.44079\n",
            "Epoch 324: loss 0.44068\n",
            "Epoch 325: loss 0.44057\n",
            "Epoch 326: loss 0.44047\n",
            "Epoch 327: loss 0.44036\n",
            "Epoch 328: loss 0.44025\n",
            "Epoch 329: loss 0.44014\n",
            "Epoch 330: loss 0.44003\n",
            "Epoch 331: loss 0.43993\n",
            "Epoch 332: loss 0.43982\n",
            "Epoch 333: loss 0.43971\n",
            "Epoch 334: loss 0.43961\n",
            "Epoch 335: loss 0.43950\n",
            "Epoch 336: loss 0.43940\n",
            "Epoch 337: loss 0.43929\n",
            "Epoch 338: loss 0.43919\n",
            "Epoch 339: loss 0.43908\n",
            "Epoch 340: loss 0.43898\n",
            "Epoch 341: loss 0.43888\n",
            "Epoch 342: loss 0.43878\n",
            "Epoch 343: loss 0.43868\n",
            "Epoch 344: loss 0.43858\n",
            "Epoch 345: loss 0.43848\n",
            "Epoch 346: loss 0.43838\n",
            "Epoch 347: loss 0.43828\n",
            "Epoch 348: loss 0.43818\n",
            "Epoch 349: loss 0.43808\n",
            "Epoch 350: loss 0.43799\n",
            "Epoch 351: loss 0.43789\n",
            "Epoch 352: loss 0.43780\n",
            "Epoch 353: loss 0.43770\n",
            "Epoch 354: loss 0.43761\n",
            "Epoch 355: loss 0.43752\n",
            "Epoch 356: loss 0.43743\n",
            "Epoch 357: loss 0.43734\n",
            "Epoch 358: loss 0.43725\n",
            "Epoch 359: loss 0.43716\n",
            "Epoch 360: loss 0.43708\n",
            "Epoch 361: loss 0.43699\n",
            "Epoch 362: loss 0.43690\n",
            "Epoch 363: loss 0.43682\n",
            "Epoch 364: loss 0.43674\n",
            "Epoch 365: loss 0.43665\n",
            "Epoch 366: loss 0.43657\n",
            "Epoch 367: loss 0.43649\n",
            "Epoch 368: loss 0.43641\n",
            "Epoch 369: loss 0.43634\n",
            "Epoch 370: loss 0.43626\n",
            "Epoch 371: loss 0.43618\n",
            "Epoch 372: loss 0.43611\n",
            "Epoch 373: loss 0.43603\n",
            "Epoch 374: loss 0.43596\n",
            "Epoch 375: loss 0.43589\n",
            "Epoch 376: loss 0.43582\n",
            "Epoch 377: loss 0.43575\n",
            "Epoch 378: loss 0.43568\n",
            "Epoch 379: loss 0.43561\n",
            "Epoch 380: loss 0.43554\n",
            "Epoch 381: loss 0.43548\n",
            "Epoch 382: loss 0.43541\n",
            "Epoch 383: loss 0.43535\n",
            "Epoch 384: loss 0.43528\n",
            "Epoch 385: loss 0.43522\n",
            "Epoch 386: loss 0.43516\n",
            "Epoch 387: loss 0.43510\n",
            "Epoch 388: loss 0.43503\n",
            "Epoch 389: loss 0.43497\n",
            "Epoch 390: loss 0.43491\n",
            "Epoch 391: loss 0.43486\n",
            "Epoch 392: loss 0.43480\n",
            "Epoch 393: loss 0.43474\n",
            "Epoch 394: loss 0.43468\n",
            "Epoch 395: loss 0.43463\n",
            "Epoch 396: loss 0.43457\n",
            "Epoch 397: loss 0.43452\n",
            "Epoch 398: loss 0.43446\n",
            "Epoch 399: loss 0.43441\n",
            "Epoch 400: loss 0.43436\n",
            "Epoch 401: loss 0.43430\n",
            "Epoch 402: loss 0.43425\n",
            "Epoch 403: loss 0.43420\n",
            "Epoch 404: loss 0.43415\n",
            "Epoch 405: loss 0.43409\n",
            "Epoch 406: loss 0.43404\n",
            "Epoch 407: loss 0.43399\n",
            "Epoch 408: loss 0.43394\n",
            "Epoch 409: loss 0.43389\n",
            "Epoch 410: loss 0.43384\n",
            "Epoch 411: loss 0.43379\n",
            "Epoch 412: loss 0.43374\n",
            "Epoch 413: loss 0.43369\n",
            "Epoch 414: loss 0.43364\n",
            "Epoch 415: loss 0.43360\n",
            "Epoch 416: loss 0.43355\n",
            "Epoch 417: loss 0.43350\n",
            "Epoch 418: loss 0.43345\n",
            "Epoch 419: loss 0.43340\n",
            "Epoch 420: loss 0.43335\n",
            "Epoch 421: loss 0.43330\n",
            "Epoch 422: loss 0.43325\n",
            "Epoch 423: loss 0.43321\n",
            "Epoch 424: loss 0.43316\n",
            "Epoch 425: loss 0.43311\n",
            "Epoch 426: loss 0.43306\n",
            "Epoch 427: loss 0.43301\n",
            "Epoch 428: loss 0.43296\n",
            "Epoch 429: loss 0.43291\n",
            "Epoch 430: loss 0.43286\n",
            "Epoch 431: loss 0.43281\n",
            "Epoch 432: loss 0.43276\n",
            "Epoch 433: loss 0.43271\n",
            "Epoch 434: loss 0.43266\n",
            "Epoch 435: loss 0.43261\n",
            "Epoch 436: loss 0.43256\n",
            "Epoch 437: loss 0.43250\n",
            "Epoch 438: loss 0.43245\n",
            "Epoch 439: loss 0.43240\n",
            "Epoch 440: loss 0.43234\n",
            "Epoch 441: loss 0.43229\n",
            "Epoch 442: loss 0.43224\n",
            "Epoch 443: loss 0.43218\n",
            "Epoch 444: loss 0.43213\n",
            "Epoch 445: loss 0.43207\n",
            "Epoch 446: loss 0.43201\n",
            "Epoch 447: loss 0.43195\n",
            "Epoch 448: loss 0.43190\n",
            "Epoch 449: loss 0.43184\n",
            "Epoch 450: loss 0.43178\n",
            "Epoch 451: loss 0.43172\n",
            "Epoch 452: loss 0.43165\n",
            "Epoch 453: loss 0.43159\n",
            "Epoch 454: loss 0.43153\n",
            "Epoch 455: loss 0.43147\n",
            "Epoch 456: loss 0.43140\n",
            "Epoch 457: loss 0.43134\n",
            "Epoch 458: loss 0.43127\n",
            "Epoch 459: loss 0.43120\n",
            "Epoch 460: loss 0.43114\n",
            "Epoch 461: loss 0.43107\n",
            "Epoch 462: loss 0.43100\n",
            "Epoch 463: loss 0.43093\n",
            "Epoch 464: loss 0.43086\n",
            "Epoch 465: loss 0.43079\n",
            "Epoch 466: loss 0.43071\n",
            "Epoch 467: loss 0.43064\n",
            "Epoch 468: loss 0.43057\n",
            "Epoch 469: loss 0.43049\n",
            "Epoch 470: loss 0.43041\n",
            "Epoch 471: loss 0.43034\n",
            "Epoch 472: loss 0.43026\n",
            "Epoch 473: loss 0.43018\n",
            "Epoch 474: loss 0.43010\n",
            "Epoch 475: loss 0.43002\n",
            "Epoch 476: loss 0.42994\n",
            "Epoch 477: loss 0.42986\n",
            "Epoch 478: loss 0.42978\n",
            "Epoch 479: loss 0.42970\n",
            "Epoch 480: loss 0.42961\n",
            "Epoch 481: loss 0.42953\n",
            "Epoch 482: loss 0.42945\n",
            "Epoch 483: loss 0.42936\n",
            "Epoch 484: loss 0.42928\n",
            "Epoch 485: loss 0.42919\n",
            "Epoch 486: loss 0.42910\n",
            "Epoch 487: loss 0.42901\n",
            "Epoch 488: loss 0.42893\n",
            "Epoch 489: loss 0.42884\n",
            "Epoch 490: loss 0.42875\n",
            "Epoch 491: loss 0.42866\n",
            "Epoch 492: loss 0.42857\n",
            "Epoch 493: loss 0.42848\n",
            "Epoch 494: loss 0.42839\n",
            "Epoch 495: loss 0.42830\n",
            "Epoch 496: loss 0.42821\n",
            "Epoch 497: loss 0.42811\n",
            "Epoch 498: loss 0.42802\n",
            "Epoch 499: loss 0.42793\n",
            "Epoch 500: loss 0.42784\n",
            "Epoch 501: loss 0.42774\n",
            "Epoch 502: loss 0.42765\n",
            "Epoch 503: loss 0.42756\n",
            "Epoch 504: loss 0.42746\n",
            "Epoch 505: loss 0.42737\n",
            "Epoch 506: loss 0.42728\n",
            "Epoch 507: loss 0.42718\n",
            "Epoch 508: loss 0.42709\n",
            "Epoch 509: loss 0.42699\n",
            "Epoch 510: loss 0.42690\n",
            "Epoch 511: loss 0.42681\n",
            "Epoch 512: loss 0.42671\n",
            "Epoch 513: loss 0.42662\n",
            "Epoch 514: loss 0.42652\n",
            "Epoch 515: loss 0.42643\n",
            "Epoch 516: loss 0.42634\n",
            "Epoch 517: loss 0.42625\n",
            "Epoch 518: loss 0.42615\n",
            "Epoch 519: loss 0.42606\n",
            "Epoch 520: loss 0.42597\n",
            "Epoch 521: loss 0.42587\n",
            "Epoch 522: loss 0.42578\n",
            "Epoch 523: loss 0.42569\n",
            "Epoch 524: loss 0.42560\n",
            "Epoch 525: loss 0.42551\n",
            "Epoch 526: loss 0.42542\n",
            "Epoch 527: loss 0.42533\n",
            "Epoch 528: loss 0.42524\n",
            "Epoch 529: loss 0.42515\n",
            "Epoch 530: loss 0.42506\n",
            "Epoch 531: loss 0.42497\n",
            "Epoch 532: loss 0.42488\n",
            "Epoch 533: loss 0.42480\n",
            "Epoch 534: loss 0.42471\n",
            "Epoch 535: loss 0.42462\n",
            "Epoch 536: loss 0.42454\n",
            "Epoch 537: loss 0.42445\n",
            "Epoch 538: loss 0.42436\n",
            "Epoch 539: loss 0.42428\n",
            "Epoch 540: loss 0.42419\n",
            "Epoch 541: loss 0.42411\n",
            "Epoch 542: loss 0.42403\n",
            "Epoch 543: loss 0.42394\n",
            "Epoch 544: loss 0.42386\n",
            "Epoch 545: loss 0.42378\n",
            "Epoch 546: loss 0.42370\n",
            "Epoch 547: loss 0.42362\n",
            "Epoch 548: loss 0.42354\n",
            "Epoch 549: loss 0.42346\n",
            "Epoch 550: loss 0.42338\n",
            "Epoch 551: loss 0.42330\n",
            "Epoch 552: loss 0.42322\n",
            "Epoch 553: loss 0.42314\n",
            "Epoch 554: loss 0.42306\n",
            "Epoch 555: loss 0.42298\n",
            "Epoch 556: loss 0.42291\n",
            "Epoch 557: loss 0.42283\n",
            "Epoch 558: loss 0.42275\n",
            "Epoch 559: loss 0.42268\n",
            "Epoch 560: loss 0.42260\n",
            "Epoch 561: loss 0.42253\n",
            "Epoch 562: loss 0.42245\n",
            "Epoch 563: loss 0.42238\n",
            "Epoch 564: loss 0.42231\n",
            "Epoch 565: loss 0.42223\n",
            "Epoch 566: loss 0.42216\n",
            "Epoch 567: loss 0.42209\n",
            "Epoch 568: loss 0.42202\n",
            "Epoch 569: loss 0.42195\n",
            "Epoch 570: loss 0.42187\n",
            "Epoch 571: loss 0.42180\n",
            "Epoch 572: loss 0.42173\n",
            "Epoch 573: loss 0.42166\n",
            "Epoch 574: loss 0.42159\n",
            "Epoch 575: loss 0.42152\n",
            "Epoch 576: loss 0.42145\n",
            "Epoch 577: loss 0.42139\n",
            "Epoch 578: loss 0.42132\n",
            "Epoch 579: loss 0.42125\n",
            "Epoch 580: loss 0.42118\n",
            "Epoch 581: loss 0.42111\n",
            "Epoch 582: loss 0.42105\n",
            "Epoch 583: loss 0.42098\n",
            "Epoch 584: loss 0.42091\n",
            "Epoch 585: loss 0.42085\n",
            "Epoch 586: loss 0.42078\n",
            "Epoch 587: loss 0.42071\n",
            "Epoch 588: loss 0.42065\n",
            "Epoch 589: loss 0.42058\n",
            "Epoch 590: loss 0.42052\n",
            "Epoch 591: loss 0.42045\n",
            "Epoch 592: loss 0.42039\n",
            "Epoch 593: loss 0.42032\n",
            "Epoch 594: loss 0.42026\n",
            "Epoch 595: loss 0.42020\n",
            "Epoch 596: loss 0.42013\n",
            "Epoch 597: loss 0.42007\n",
            "Epoch 598: loss 0.42001\n",
            "Epoch 599: loss 0.41994\n",
            "Epoch 600: loss 0.41988\n",
            "Epoch 601: loss 0.41982\n",
            "Epoch 602: loss 0.41976\n",
            "Epoch 603: loss 0.41969\n",
            "Epoch 604: loss 0.41963\n",
            "Epoch 605: loss 0.41957\n",
            "Epoch 606: loss 0.41951\n",
            "Epoch 607: loss 0.41945\n",
            "Epoch 608: loss 0.41939\n",
            "Epoch 609: loss 0.41933\n",
            "Epoch 610: loss 0.41927\n",
            "Epoch 611: loss 0.41921\n",
            "Epoch 612: loss 0.41915\n",
            "Epoch 613: loss 0.41909\n",
            "Epoch 614: loss 0.41903\n",
            "Epoch 615: loss 0.41897\n",
            "Epoch 616: loss 0.41891\n",
            "Epoch 617: loss 0.41885\n",
            "Epoch 618: loss 0.41879\n",
            "Epoch 619: loss 0.41873\n",
            "Epoch 620: loss 0.41867\n",
            "Epoch 621: loss 0.41861\n",
            "Epoch 622: loss 0.41855\n",
            "Epoch 623: loss 0.41849\n",
            "Epoch 624: loss 0.41843\n",
            "Epoch 625: loss 0.41838\n",
            "Epoch 626: loss 0.41832\n",
            "Epoch 627: loss 0.41826\n",
            "Epoch 628: loss 0.41820\n",
            "Epoch 629: loss 0.41814\n",
            "Epoch 630: loss 0.41808\n",
            "Epoch 631: loss 0.41803\n",
            "Epoch 632: loss 0.41797\n",
            "Epoch 633: loss 0.41791\n",
            "Epoch 634: loss 0.41785\n",
            "Epoch 635: loss 0.41780\n",
            "Epoch 636: loss 0.41774\n",
            "Epoch 637: loss 0.41768\n",
            "Epoch 638: loss 0.41762\n",
            "Epoch 639: loss 0.41757\n",
            "Epoch 640: loss 0.41751\n",
            "Epoch 641: loss 0.41745\n",
            "Epoch 642: loss 0.41739\n",
            "Epoch 643: loss 0.41734\n",
            "Epoch 644: loss 0.41728\n",
            "Epoch 645: loss 0.41722\n",
            "Epoch 646: loss 0.41716\n",
            "Epoch 647: loss 0.41711\n",
            "Epoch 648: loss 0.41705\n",
            "Epoch 649: loss 0.41699\n",
            "Epoch 650: loss 0.41693\n",
            "Epoch 651: loss 0.41688\n",
            "Epoch 652: loss 0.41682\n",
            "Epoch 653: loss 0.41676\n",
            "Epoch 654: loss 0.41670\n",
            "Epoch 655: loss 0.41664\n",
            "Epoch 656: loss 0.41658\n",
            "Epoch 657: loss 0.41653\n",
            "Epoch 658: loss 0.41647\n",
            "Epoch 659: loss 0.41641\n",
            "Epoch 660: loss 0.41635\n",
            "Epoch 661: loss 0.41629\n",
            "Epoch 662: loss 0.41623\n",
            "Epoch 663: loss 0.41617\n",
            "Epoch 664: loss 0.41611\n",
            "Epoch 665: loss 0.41605\n",
            "Epoch 666: loss 0.41599\n",
            "Epoch 667: loss 0.41593\n",
            "Epoch 668: loss 0.41587\n",
            "Epoch 669: loss 0.41581\n",
            "Epoch 670: loss 0.41575\n",
            "Epoch 671: loss 0.41568\n",
            "Epoch 672: loss 0.41562\n",
            "Epoch 673: loss 0.41556\n",
            "Epoch 674: loss 0.41549\n",
            "Epoch 675: loss 0.41543\n",
            "Epoch 676: loss 0.41536\n",
            "Epoch 677: loss 0.41530\n",
            "Epoch 678: loss 0.41523\n",
            "Epoch 679: loss 0.41517\n",
            "Epoch 680: loss 0.41510\n",
            "Epoch 681: loss 0.41503\n",
            "Epoch 682: loss 0.41497\n",
            "Epoch 683: loss 0.41490\n",
            "Epoch 684: loss 0.41483\n",
            "Epoch 685: loss 0.41476\n",
            "Epoch 686: loss 0.41468\n",
            "Epoch 687: loss 0.41461\n",
            "Epoch 688: loss 0.41454\n",
            "Epoch 689: loss 0.41447\n",
            "Epoch 690: loss 0.41439\n",
            "Epoch 691: loss 0.41431\n",
            "Epoch 692: loss 0.41424\n",
            "Epoch 693: loss 0.41416\n",
            "Epoch 694: loss 0.41408\n",
            "Epoch 695: loss 0.41400\n",
            "Epoch 696: loss 0.41392\n",
            "Epoch 697: loss 0.41384\n",
            "Epoch 698: loss 0.41375\n",
            "Epoch 699: loss 0.41367\n",
            "Epoch 700: loss 0.41358\n",
            "Epoch 701: loss 0.41349\n",
            "Epoch 702: loss 0.41340\n",
            "Epoch 703: loss 0.41331\n",
            "Epoch 704: loss 0.41322\n",
            "Epoch 705: loss 0.41313\n",
            "Epoch 706: loss 0.41303\n",
            "Epoch 707: loss 0.41294\n",
            "Epoch 708: loss 0.41284\n",
            "Epoch 709: loss 0.41274\n",
            "Epoch 710: loss 0.41264\n",
            "Epoch 711: loss 0.41254\n",
            "Epoch 712: loss 0.41243\n",
            "Epoch 713: loss 0.41233\n",
            "Epoch 714: loss 0.41222\n",
            "Epoch 715: loss 0.41211\n",
            "Epoch 716: loss 0.41200\n",
            "Epoch 717: loss 0.41189\n",
            "Epoch 718: loss 0.41178\n",
            "Epoch 719: loss 0.41167\n",
            "Epoch 720: loss 0.41155\n",
            "Epoch 721: loss 0.41144\n",
            "Epoch 722: loss 0.41132\n",
            "Epoch 723: loss 0.41120\n",
            "Epoch 724: loss 0.41108\n",
            "Epoch 725: loss 0.41096\n",
            "Epoch 726: loss 0.41084\n",
            "Epoch 727: loss 0.41072\n",
            "Epoch 728: loss 0.41060\n",
            "Epoch 729: loss 0.41047\n",
            "Epoch 730: loss 0.41035\n",
            "Epoch 731: loss 0.41023\n",
            "Epoch 732: loss 0.41010\n",
            "Epoch 733: loss 0.40998\n",
            "Epoch 734: loss 0.40985\n",
            "Epoch 735: loss 0.40973\n",
            "Epoch 736: loss 0.40960\n",
            "Epoch 737: loss 0.40947\n",
            "Epoch 738: loss 0.40935\n",
            "Epoch 739: loss 0.40922\n",
            "Epoch 740: loss 0.40910\n",
            "Epoch 741: loss 0.40897\n",
            "Epoch 742: loss 0.40885\n",
            "Epoch 743: loss 0.40872\n",
            "Epoch 744: loss 0.40860\n",
            "Epoch 745: loss 0.40847\n",
            "Epoch 746: loss 0.40835\n",
            "Epoch 747: loss 0.40823\n",
            "Epoch 748: loss 0.40810\n",
            "Epoch 749: loss 0.40798\n",
            "Epoch 750: loss 0.40786\n",
            "Epoch 751: loss 0.40774\n",
            "Epoch 752: loss 0.40762\n",
            "Epoch 753: loss 0.40750\n",
            "Epoch 754: loss 0.40738\n",
            "Epoch 755: loss 0.40726\n",
            "Epoch 756: loss 0.40714\n",
            "Epoch 757: loss 0.40703\n",
            "Epoch 758: loss 0.40691\n",
            "Epoch 759: loss 0.40679\n",
            "Epoch 760: loss 0.40668\n",
            "Epoch 761: loss 0.40656\n",
            "Epoch 762: loss 0.40645\n",
            "Epoch 763: loss 0.40634\n",
            "Epoch 764: loss 0.40623\n",
            "Epoch 765: loss 0.40612\n",
            "Epoch 766: loss 0.40601\n",
            "Epoch 767: loss 0.40590\n",
            "Epoch 768: loss 0.40579\n",
            "Epoch 769: loss 0.40568\n",
            "Epoch 770: loss 0.40558\n",
            "Epoch 771: loss 0.40547\n",
            "Epoch 772: loss 0.40537\n",
            "Epoch 773: loss 0.40526\n",
            "Epoch 774: loss 0.40516\n",
            "Epoch 775: loss 0.40506\n",
            "Epoch 776: loss 0.40496\n",
            "Epoch 777: loss 0.40486\n",
            "Epoch 778: loss 0.40476\n",
            "Epoch 779: loss 0.40466\n",
            "Epoch 780: loss 0.40456\n",
            "Epoch 781: loss 0.40446\n",
            "Epoch 782: loss 0.40437\n",
            "Epoch 783: loss 0.40427\n",
            "Epoch 784: loss 0.40418\n",
            "Epoch 785: loss 0.40408\n",
            "Epoch 786: loss 0.40399\n",
            "Epoch 787: loss 0.40390\n",
            "Epoch 788: loss 0.40381\n",
            "Epoch 789: loss 0.40371\n",
            "Epoch 790: loss 0.40362\n",
            "Epoch 791: loss 0.40354\n",
            "Epoch 792: loss 0.40345\n",
            "Epoch 793: loss 0.40336\n",
            "Epoch 794: loss 0.40327\n",
            "Epoch 795: loss 0.40318\n",
            "Epoch 796: loss 0.40310\n",
            "Epoch 797: loss 0.40301\n",
            "Epoch 798: loss 0.40293\n",
            "Epoch 799: loss 0.40285\n",
            "Epoch 800: loss 0.40276\n",
            "Epoch 801: loss 0.40268\n",
            "Epoch 802: loss 0.40260\n",
            "Epoch 803: loss 0.40252\n",
            "Epoch 804: loss 0.40243\n",
            "Epoch 805: loss 0.40235\n",
            "Epoch 806: loss 0.40227\n",
            "Epoch 807: loss 0.40220\n",
            "Epoch 808: loss 0.40212\n",
            "Epoch 809: loss 0.40204\n",
            "Epoch 810: loss 0.40196\n",
            "Epoch 811: loss 0.40188\n",
            "Epoch 812: loss 0.40181\n",
            "Epoch 813: loss 0.40173\n",
            "Epoch 814: loss 0.40166\n",
            "Epoch 815: loss 0.40158\n",
            "Epoch 816: loss 0.40151\n",
            "Epoch 817: loss 0.40143\n",
            "Epoch 818: loss 0.40136\n",
            "Epoch 819: loss 0.40129\n",
            "Epoch 820: loss 0.40121\n",
            "Epoch 821: loss 0.40114\n",
            "Epoch 822: loss 0.40107\n",
            "Epoch 823: loss 0.40100\n",
            "Epoch 824: loss 0.40093\n",
            "Epoch 825: loss 0.40086\n",
            "Epoch 826: loss 0.40079\n",
            "Epoch 827: loss 0.40072\n",
            "Epoch 828: loss 0.40065\n",
            "Epoch 829: loss 0.40058\n",
            "Epoch 830: loss 0.40051\n",
            "Epoch 831: loss 0.40044\n",
            "Epoch 832: loss 0.40037\n",
            "Epoch 833: loss 0.40030\n",
            "Epoch 834: loss 0.40023\n",
            "Epoch 835: loss 0.40017\n",
            "Epoch 836: loss 0.40010\n",
            "Epoch 837: loss 0.40003\n",
            "Epoch 838: loss 0.39997\n",
            "Epoch 839: loss 0.39990\n",
            "Epoch 840: loss 0.39983\n",
            "Epoch 841: loss 0.39977\n",
            "Epoch 842: loss 0.39970\n",
            "Epoch 843: loss 0.39964\n",
            "Epoch 844: loss 0.39957\n",
            "Epoch 845: loss 0.39951\n",
            "Epoch 846: loss 0.39944\n",
            "Epoch 847: loss 0.39938\n",
            "Epoch 848: loss 0.39931\n",
            "Epoch 849: loss 0.39925\n",
            "Epoch 850: loss 0.39919\n",
            "Epoch 851: loss 0.39912\n",
            "Epoch 852: loss 0.39906\n",
            "Epoch 853: loss 0.39899\n",
            "Epoch 854: loss 0.39893\n",
            "Epoch 855: loss 0.39887\n",
            "Epoch 856: loss 0.39881\n",
            "Epoch 857: loss 0.39874\n",
            "Epoch 858: loss 0.39868\n",
            "Epoch 859: loss 0.39862\n",
            "Epoch 860: loss 0.39856\n",
            "Epoch 861: loss 0.39849\n",
            "Epoch 862: loss 0.39843\n",
            "Epoch 863: loss 0.39837\n",
            "Epoch 864: loss 0.39831\n",
            "Epoch 865: loss 0.39825\n",
            "Epoch 866: loss 0.39819\n",
            "Epoch 867: loss 0.39812\n",
            "Epoch 868: loss 0.39806\n",
            "Epoch 869: loss 0.39800\n",
            "Epoch 870: loss 0.39794\n",
            "Epoch 871: loss 0.39788\n",
            "Epoch 872: loss 0.39782\n",
            "Epoch 873: loss 0.39776\n",
            "Epoch 874: loss 0.39770\n",
            "Epoch 875: loss 0.39764\n",
            "Epoch 876: loss 0.39758\n",
            "Epoch 877: loss 0.39752\n",
            "Epoch 878: loss 0.39746\n",
            "Epoch 879: loss 0.39740\n",
            "Epoch 880: loss 0.39734\n",
            "Epoch 881: loss 0.39728\n",
            "Epoch 882: loss 0.39722\n",
            "Epoch 883: loss 0.39716\n",
            "Epoch 884: loss 0.39710\n",
            "Epoch 885: loss 0.39704\n",
            "Epoch 886: loss 0.39698\n",
            "Epoch 887: loss 0.39692\n",
            "Epoch 888: loss 0.39686\n",
            "Epoch 889: loss 0.39680\n",
            "Epoch 890: loss 0.39674\n",
            "Epoch 891: loss 0.39668\n",
            "Epoch 892: loss 0.39662\n",
            "Epoch 893: loss 0.39656\n",
            "Epoch 894: loss 0.39650\n",
            "Epoch 895: loss 0.39644\n",
            "Epoch 896: loss 0.39638\n",
            "Epoch 897: loss 0.39632\n",
            "Epoch 898: loss 0.39627\n",
            "Epoch 899: loss 0.39621\n",
            "Epoch 900: loss 0.39615\n",
            "Epoch 901: loss 0.39609\n",
            "Epoch 902: loss 0.39603\n",
            "Epoch 903: loss 0.39597\n",
            "Epoch 904: loss 0.39591\n",
            "Epoch 905: loss 0.39585\n",
            "Epoch 906: loss 0.39580\n",
            "Epoch 907: loss 0.39574\n",
            "Epoch 908: loss 0.39568\n",
            "Epoch 909: loss 0.39562\n",
            "Epoch 910: loss 0.39556\n",
            "Epoch 911: loss 0.39551\n",
            "Epoch 912: loss 0.39545\n",
            "Epoch 913: loss 0.39539\n",
            "Epoch 914: loss 0.39533\n",
            "Epoch 915: loss 0.39527\n",
            "Epoch 916: loss 0.39522\n",
            "Epoch 917: loss 0.39516\n",
            "Epoch 918: loss 0.39510\n",
            "Epoch 919: loss 0.39504\n",
            "Epoch 920: loss 0.39499\n",
            "Epoch 921: loss 0.39493\n",
            "Epoch 922: loss 0.39487\n",
            "Epoch 923: loss 0.39481\n",
            "Epoch 924: loss 0.39476\n",
            "Epoch 925: loss 0.39470\n",
            "Epoch 926: loss 0.39464\n",
            "Epoch 927: loss 0.39459\n",
            "Epoch 928: loss 0.39453\n",
            "Epoch 929: loss 0.39447\n",
            "Epoch 930: loss 0.39442\n",
            "Epoch 931: loss 0.39436\n",
            "Epoch 932: loss 0.39430\n",
            "Epoch 933: loss 0.39425\n",
            "Epoch 934: loss 0.39419\n",
            "Epoch 935: loss 0.39414\n",
            "Epoch 936: loss 0.39408\n",
            "Epoch 937: loss 0.39402\n",
            "Epoch 938: loss 0.39397\n",
            "Epoch 939: loss 0.39391\n",
            "Epoch 940: loss 0.39386\n",
            "Epoch 941: loss 0.39380\n",
            "Epoch 942: loss 0.39375\n",
            "Epoch 943: loss 0.39369\n",
            "Epoch 944: loss 0.39364\n",
            "Epoch 945: loss 0.39358\n",
            "Epoch 946: loss 0.39353\n",
            "Epoch 947: loss 0.39347\n",
            "Epoch 948: loss 0.39342\n",
            "Epoch 949: loss 0.39336\n",
            "Epoch 950: loss 0.39331\n",
            "Epoch 951: loss 0.39325\n",
            "Epoch 952: loss 0.39320\n",
            "Epoch 953: loss 0.39315\n",
            "Epoch 954: loss 0.39309\n",
            "Epoch 955: loss 0.39304\n",
            "Epoch 956: loss 0.39299\n",
            "Epoch 957: loss 0.39293\n",
            "Epoch 958: loss 0.39288\n",
            "Epoch 959: loss 0.39283\n",
            "Epoch 960: loss 0.39277\n",
            "Epoch 961: loss 0.39272\n",
            "Epoch 962: loss 0.39267\n",
            "Epoch 963: loss 0.39261\n",
            "Epoch 964: loss 0.39256\n",
            "Epoch 965: loss 0.39251\n",
            "Epoch 966: loss 0.39246\n",
            "Epoch 967: loss 0.39240\n",
            "Epoch 968: loss 0.39235\n",
            "Epoch 969: loss 0.39230\n",
            "Epoch 970: loss 0.39225\n",
            "Epoch 971: loss 0.39220\n",
            "Epoch 972: loss 0.39215\n",
            "Epoch 973: loss 0.39210\n",
            "Epoch 974: loss 0.39204\n",
            "Epoch 975: loss 0.39199\n",
            "Epoch 976: loss 0.39194\n",
            "Epoch 977: loss 0.39189\n",
            "Epoch 978: loss 0.39184\n",
            "Epoch 979: loss 0.39179\n",
            "Epoch 980: loss 0.39174\n",
            "Epoch 981: loss 0.39169\n",
            "Epoch 982: loss 0.39164\n",
            "Epoch 983: loss 0.39159\n",
            "Epoch 984: loss 0.39154\n",
            "Epoch 985: loss 0.39149\n",
            "Epoch 986: loss 0.39144\n",
            "Epoch 987: loss 0.39139\n",
            "Epoch 988: loss 0.39134\n",
            "Epoch 989: loss 0.39130\n",
            "Epoch 990: loss 0.39125\n",
            "Epoch 991: loss 0.39120\n",
            "Epoch 992: loss 0.39115\n",
            "Epoch 993: loss 0.39110\n",
            "Epoch 994: loss 0.39105\n",
            "Epoch 995: loss 0.39100\n",
            "Epoch 996: loss 0.39096\n",
            "Epoch 997: loss 0.39091\n",
            "Epoch 998: loss 0.39086\n",
            "Epoch 999: loss 0.39081\n",
            "Epoch 1000: loss 0.39077\n",
            "Epoch 1001: loss 0.39072\n",
            "Epoch 1002: loss 0.39067\n",
            "Epoch 1003: loss 0.39063\n",
            "Epoch 1004: loss 0.39058\n",
            "Epoch 1005: loss 0.39053\n",
            "Epoch 1006: loss 0.39048\n",
            "Epoch 1007: loss 0.39044\n",
            "Epoch 1008: loss 0.39039\n",
            "Epoch 1009: loss 0.39035\n",
            "Epoch 1010: loss 0.39030\n",
            "Epoch 1011: loss 0.39025\n",
            "Epoch 1012: loss 0.39021\n",
            "Epoch 1013: loss 0.39016\n",
            "Epoch 1014: loss 0.39012\n",
            "Epoch 1015: loss 0.39007\n",
            "Epoch 1016: loss 0.39003\n",
            "Epoch 1017: loss 0.38998\n",
            "Epoch 1018: loss 0.38994\n",
            "Epoch 1019: loss 0.38989\n",
            "Epoch 1020: loss 0.38985\n",
            "Epoch 1021: loss 0.38980\n",
            "Epoch 1022: loss 0.38976\n",
            "Epoch 1023: loss 0.38971\n",
            "Epoch 1024: loss 0.38967\n",
            "Epoch 1025: loss 0.38962\n",
            "Epoch 1026: loss 0.38958\n",
            "Epoch 1027: loss 0.38953\n",
            "Epoch 1028: loss 0.38949\n",
            "Epoch 1029: loss 0.38944\n",
            "Epoch 1030: loss 0.38940\n",
            "Epoch 1031: loss 0.38936\n",
            "Epoch 1032: loss 0.38931\n",
            "Epoch 1033: loss 0.38927\n",
            "Epoch 1034: loss 0.38922\n",
            "Epoch 1035: loss 0.38918\n",
            "Epoch 1036: loss 0.38914\n",
            "Epoch 1037: loss 0.38909\n",
            "Epoch 1038: loss 0.38905\n",
            "Epoch 1039: loss 0.38901\n",
            "Epoch 1040: loss 0.38896\n",
            "Epoch 1041: loss 0.38892\n",
            "Epoch 1042: loss 0.38888\n",
            "Epoch 1043: loss 0.38883\n",
            "Epoch 1044: loss 0.38879\n",
            "Epoch 1045: loss 0.38874\n",
            "Epoch 1046: loss 0.38870\n",
            "Epoch 1047: loss 0.38866\n",
            "Epoch 1048: loss 0.38861\n",
            "Epoch 1049: loss 0.38857\n",
            "Epoch 1050: loss 0.38853\n",
            "Epoch 1051: loss 0.38848\n",
            "Epoch 1052: loss 0.38844\n",
            "Epoch 1053: loss 0.38840\n",
            "Epoch 1054: loss 0.38835\n",
            "Epoch 1055: loss 0.38831\n",
            "Epoch 1056: loss 0.38826\n",
            "Epoch 1057: loss 0.38822\n",
            "Epoch 1058: loss 0.38817\n",
            "Epoch 1059: loss 0.38813\n",
            "Epoch 1060: loss 0.38808\n",
            "Epoch 1061: loss 0.38804\n",
            "Epoch 1062: loss 0.38799\n",
            "Epoch 1063: loss 0.38795\n",
            "Epoch 1064: loss 0.38790\n",
            "Epoch 1065: loss 0.38786\n",
            "Epoch 1066: loss 0.38781\n",
            "Epoch 1067: loss 0.38777\n",
            "Epoch 1068: loss 0.38772\n",
            "Epoch 1069: loss 0.38767\n",
            "Epoch 1070: loss 0.38763\n",
            "Epoch 1071: loss 0.38758\n",
            "Epoch 1072: loss 0.38753\n",
            "Epoch 1073: loss 0.38748\n",
            "Epoch 1074: loss 0.38743\n",
            "Epoch 1075: loss 0.38739\n",
            "Epoch 1076: loss 0.38734\n",
            "Epoch 1077: loss 0.38729\n",
            "Epoch 1078: loss 0.38724\n",
            "Epoch 1079: loss 0.38719\n",
            "Epoch 1080: loss 0.38713\n",
            "Epoch 1081: loss 0.38708\n",
            "Epoch 1082: loss 0.38703\n",
            "Epoch 1083: loss 0.38698\n",
            "Epoch 1084: loss 0.38692\n",
            "Epoch 1085: loss 0.38687\n",
            "Epoch 1086: loss 0.38682\n",
            "Epoch 1087: loss 0.38676\n",
            "Epoch 1088: loss 0.38671\n",
            "Epoch 1089: loss 0.38665\n",
            "Epoch 1090: loss 0.38659\n",
            "Epoch 1091: loss 0.38654\n",
            "Epoch 1092: loss 0.38648\n",
            "Epoch 1093: loss 0.38642\n",
            "Epoch 1094: loss 0.38636\n",
            "Epoch 1095: loss 0.38630\n",
            "Epoch 1096: loss 0.38625\n",
            "Epoch 1097: loss 0.38619\n",
            "Epoch 1098: loss 0.38613\n",
            "Epoch 1099: loss 0.38607\n",
            "Epoch 1100: loss 0.38601\n",
            "Epoch 1101: loss 0.38594\n",
            "Epoch 1102: loss 0.38588\n",
            "Epoch 1103: loss 0.38582\n",
            "Epoch 1104: loss 0.38576\n",
            "Epoch 1105: loss 0.38570\n",
            "Epoch 1106: loss 0.38564\n",
            "Epoch 1107: loss 0.38558\n",
            "Epoch 1108: loss 0.38552\n",
            "Epoch 1109: loss 0.38545\n",
            "Epoch 1110: loss 0.38539\n",
            "Epoch 1111: loss 0.38533\n",
            "Epoch 1112: loss 0.38527\n",
            "Epoch 1113: loss 0.38521\n",
            "Epoch 1114: loss 0.38515\n",
            "Epoch 1115: loss 0.38509\n",
            "Epoch 1116: loss 0.38503\n",
            "Epoch 1117: loss 0.38497\n",
            "Epoch 1118: loss 0.38491\n",
            "Epoch 1119: loss 0.38485\n",
            "Epoch 1120: loss 0.38479\n",
            "Epoch 1121: loss 0.38473\n",
            "Epoch 1122: loss 0.38467\n",
            "Epoch 1123: loss 0.38461\n",
            "Epoch 1124: loss 0.38455\n",
            "Epoch 1125: loss 0.38450\n",
            "Epoch 1126: loss 0.38444\n",
            "Epoch 1127: loss 0.38438\n",
            "Epoch 1128: loss 0.38432\n",
            "Epoch 1129: loss 0.38427\n",
            "Epoch 1130: loss 0.38421\n",
            "Epoch 1131: loss 0.38415\n",
            "Epoch 1132: loss 0.38410\n",
            "Epoch 1133: loss 0.38404\n",
            "Epoch 1134: loss 0.38399\n",
            "Epoch 1135: loss 0.38393\n",
            "Epoch 1136: loss 0.38388\n",
            "Epoch 1137: loss 0.38382\n",
            "Epoch 1138: loss 0.38377\n",
            "Epoch 1139: loss 0.38371\n",
            "Epoch 1140: loss 0.38366\n",
            "Epoch 1141: loss 0.38361\n",
            "Epoch 1142: loss 0.38355\n",
            "Epoch 1143: loss 0.38350\n",
            "Epoch 1144: loss 0.38345\n",
            "Epoch 1145: loss 0.38340\n",
            "Epoch 1146: loss 0.38334\n",
            "Epoch 1147: loss 0.38329\n",
            "Epoch 1148: loss 0.38324\n",
            "Epoch 1149: loss 0.38319\n",
            "Epoch 1150: loss 0.38314\n",
            "Epoch 1151: loss 0.38309\n",
            "Epoch 1152: loss 0.38303\n",
            "Epoch 1153: loss 0.38298\n",
            "Epoch 1154: loss 0.38293\n",
            "Epoch 1155: loss 0.38288\n",
            "Epoch 1156: loss 0.38283\n",
            "Epoch 1157: loss 0.38278\n",
            "Epoch 1158: loss 0.38273\n",
            "Epoch 1159: loss 0.38268\n",
            "Epoch 1160: loss 0.38264\n",
            "Epoch 1161: loss 0.38259\n",
            "Epoch 1162: loss 0.38254\n",
            "Epoch 1163: loss 0.38249\n",
            "Epoch 1164: loss 0.38244\n",
            "Epoch 1165: loss 0.38239\n",
            "Epoch 1166: loss 0.38234\n",
            "Epoch 1167: loss 0.38230\n",
            "Epoch 1168: loss 0.38225\n",
            "Epoch 1169: loss 0.38220\n",
            "Epoch 1170: loss 0.38215\n",
            "Epoch 1171: loss 0.38211\n",
            "Epoch 1172: loss 0.38206\n",
            "Epoch 1173: loss 0.38201\n",
            "Epoch 1174: loss 0.38197\n",
            "Epoch 1175: loss 0.38192\n",
            "Epoch 1176: loss 0.38188\n",
            "Epoch 1177: loss 0.38183\n",
            "Epoch 1178: loss 0.38178\n",
            "Epoch 1179: loss 0.38174\n",
            "Epoch 1180: loss 0.38169\n",
            "Epoch 1181: loss 0.38165\n",
            "Epoch 1182: loss 0.38160\n",
            "Epoch 1183: loss 0.38156\n",
            "Epoch 1184: loss 0.38151\n",
            "Epoch 1185: loss 0.38147\n",
            "Epoch 1186: loss 0.38143\n",
            "Epoch 1187: loss 0.38138\n",
            "Epoch 1188: loss 0.38134\n",
            "Epoch 1189: loss 0.38129\n",
            "Epoch 1190: loss 0.38125\n",
            "Epoch 1191: loss 0.38121\n",
            "Epoch 1192: loss 0.38116\n",
            "Epoch 1193: loss 0.38112\n",
            "Epoch 1194: loss 0.38108\n",
            "Epoch 1195: loss 0.38104\n",
            "Epoch 1196: loss 0.38099\n",
            "Epoch 1197: loss 0.38095\n",
            "Epoch 1198: loss 0.38091\n",
            "Epoch 1199: loss 0.38087\n",
            "Epoch 1200: loss 0.38083\n",
            "Epoch 1201: loss 0.38078\n",
            "Epoch 1202: loss 0.38074\n",
            "Epoch 1203: loss 0.38070\n",
            "Epoch 1204: loss 0.38066\n",
            "Epoch 1205: loss 0.38062\n",
            "Epoch 1206: loss 0.38058\n",
            "Epoch 1207: loss 0.38054\n",
            "Epoch 1208: loss 0.38050\n",
            "Epoch 1209: loss 0.38046\n",
            "Epoch 1210: loss 0.38042\n",
            "Epoch 1211: loss 0.38038\n",
            "Epoch 1212: loss 0.38034\n",
            "Epoch 1213: loss 0.38030\n",
            "Epoch 1214: loss 0.38026\n",
            "Epoch 1215: loss 0.38022\n",
            "Epoch 1216: loss 0.38018\n",
            "Epoch 1217: loss 0.38014\n",
            "Epoch 1218: loss 0.38010\n",
            "Epoch 1219: loss 0.38006\n",
            "Epoch 1220: loss 0.38002\n",
            "Epoch 1221: loss 0.37998\n",
            "Epoch 1222: loss 0.37994\n",
            "Epoch 1223: loss 0.37991\n",
            "Epoch 1224: loss 0.37987\n",
            "Epoch 1225: loss 0.37983\n",
            "Epoch 1226: loss 0.37979\n",
            "Epoch 1227: loss 0.37975\n",
            "Epoch 1228: loss 0.37972\n",
            "Epoch 1229: loss 0.37968\n",
            "Epoch 1230: loss 0.37964\n",
            "Epoch 1231: loss 0.37961\n",
            "Epoch 1232: loss 0.37957\n",
            "Epoch 1233: loss 0.37953\n",
            "Epoch 1234: loss 0.37949\n",
            "Epoch 1235: loss 0.37946\n",
            "Epoch 1236: loss 0.37942\n",
            "Epoch 1237: loss 0.37939\n",
            "Epoch 1238: loss 0.37935\n",
            "Epoch 1239: loss 0.37931\n",
            "Epoch 1240: loss 0.37928\n",
            "Epoch 1241: loss 0.37924\n",
            "Epoch 1242: loss 0.37921\n",
            "Epoch 1243: loss 0.37917\n",
            "Epoch 1244: loss 0.37913\n",
            "Epoch 1245: loss 0.37910\n",
            "Epoch 1246: loss 0.37906\n",
            "Epoch 1247: loss 0.37903\n",
            "Epoch 1248: loss 0.37899\n",
            "Epoch 1249: loss 0.37896\n",
            "Epoch 1250: loss 0.37892\n",
            "Epoch 1251: loss 0.37889\n",
            "Epoch 1252: loss 0.37886\n",
            "Epoch 1253: loss 0.37882\n",
            "Epoch 1254: loss 0.37879\n",
            "Epoch 1255: loss 0.37875\n",
            "Epoch 1256: loss 0.37872\n",
            "Epoch 1257: loss 0.37868\n",
            "Epoch 1258: loss 0.37865\n",
            "Epoch 1259: loss 0.37862\n",
            "Epoch 1260: loss 0.37858\n",
            "Epoch 1261: loss 0.37855\n",
            "Epoch 1262: loss 0.37852\n",
            "Epoch 1263: loss 0.37848\n",
            "Epoch 1264: loss 0.37845\n",
            "Epoch 1265: loss 0.37842\n",
            "Epoch 1266: loss 0.37839\n",
            "Epoch 1267: loss 0.37835\n",
            "Epoch 1268: loss 0.37832\n",
            "Epoch 1269: loss 0.37829\n",
            "Epoch 1270: loss 0.37825\n",
            "Epoch 1271: loss 0.37822\n",
            "Epoch 1272: loss 0.37819\n",
            "Epoch 1273: loss 0.37816\n",
            "Epoch 1274: loss 0.37813\n",
            "Epoch 1275: loss 0.37809\n",
            "Epoch 1276: loss 0.37806\n",
            "Epoch 1277: loss 0.37803\n",
            "Epoch 1278: loss 0.37800\n",
            "Epoch 1279: loss 0.37797\n",
            "Epoch 1280: loss 0.37794\n",
            "Epoch 1281: loss 0.37790\n",
            "Epoch 1282: loss 0.37787\n",
            "Epoch 1283: loss 0.37784\n",
            "Epoch 1284: loss 0.37781\n",
            "Epoch 1285: loss 0.37778\n",
            "Epoch 1286: loss 0.37775\n",
            "Epoch 1287: loss 0.37772\n",
            "Epoch 1288: loss 0.37769\n",
            "Epoch 1289: loss 0.37766\n",
            "Epoch 1290: loss 0.37762\n",
            "Epoch 1291: loss 0.37759\n",
            "Epoch 1292: loss 0.37756\n",
            "Epoch 1293: loss 0.37753\n",
            "Epoch 1294: loss 0.37750\n",
            "Epoch 1295: loss 0.37747\n",
            "Epoch 1296: loss 0.37744\n",
            "Epoch 1297: loss 0.37741\n",
            "Epoch 1298: loss 0.37738\n",
            "Epoch 1299: loss 0.37735\n",
            "Epoch 1300: loss 0.37732\n",
            "Epoch 1301: loss 0.37729\n",
            "Epoch 1302: loss 0.37726\n",
            "Epoch 1303: loss 0.37723\n",
            "Epoch 1304: loss 0.37720\n",
            "Epoch 1305: loss 0.37717\n",
            "Epoch 1306: loss 0.37714\n",
            "Epoch 1307: loss 0.37711\n",
            "Epoch 1308: loss 0.37708\n",
            "Epoch 1309: loss 0.37706\n",
            "Epoch 1310: loss 0.37703\n",
            "Epoch 1311: loss 0.37700\n",
            "Epoch 1312: loss 0.37697\n",
            "Epoch 1313: loss 0.37694\n",
            "Epoch 1314: loss 0.37691\n",
            "Epoch 1315: loss 0.37688\n",
            "Epoch 1316: loss 0.37685\n",
            "Epoch 1317: loss 0.37682\n",
            "Epoch 1318: loss 0.37679\n",
            "Epoch 1319: loss 0.37676\n",
            "Epoch 1320: loss 0.37674\n",
            "Epoch 1321: loss 0.37671\n",
            "Epoch 1322: loss 0.37668\n",
            "Epoch 1323: loss 0.37665\n",
            "Epoch 1324: loss 0.37662\n",
            "Epoch 1325: loss 0.37659\n",
            "Epoch 1326: loss 0.37656\n",
            "Epoch 1327: loss 0.37654\n",
            "Epoch 1328: loss 0.37651\n",
            "Epoch 1329: loss 0.37648\n",
            "Epoch 1330: loss 0.37645\n",
            "Epoch 1331: loss 0.37642\n",
            "Epoch 1332: loss 0.37639\n",
            "Epoch 1333: loss 0.37637\n",
            "Epoch 1334: loss 0.37634\n",
            "Epoch 1335: loss 0.37631\n",
            "Epoch 1336: loss 0.37628\n",
            "Epoch 1337: loss 0.37625\n",
            "Epoch 1338: loss 0.37622\n",
            "Epoch 1339: loss 0.37620\n",
            "Epoch 1340: loss 0.37617\n",
            "Epoch 1341: loss 0.37614\n",
            "Epoch 1342: loss 0.37611\n",
            "Epoch 1343: loss 0.37608\n",
            "Epoch 1344: loss 0.37606\n",
            "Epoch 1345: loss 0.37603\n",
            "Epoch 1346: loss 0.37600\n",
            "Epoch 1347: loss 0.37597\n",
            "Epoch 1348: loss 0.37594\n",
            "Epoch 1349: loss 0.37592\n",
            "Epoch 1350: loss 0.37589\n",
            "Epoch 1351: loss 0.37586\n",
            "Epoch 1352: loss 0.37583\n",
            "Epoch 1353: loss 0.37581\n",
            "Epoch 1354: loss 0.37578\n",
            "Epoch 1355: loss 0.37575\n",
            "Epoch 1356: loss 0.37572\n",
            "Epoch 1357: loss 0.37570\n",
            "Epoch 1358: loss 0.37567\n",
            "Epoch 1359: loss 0.37564\n",
            "Epoch 1360: loss 0.37561\n",
            "Epoch 1361: loss 0.37558\n",
            "Epoch 1362: loss 0.37556\n",
            "Epoch 1363: loss 0.37553\n",
            "Epoch 1364: loss 0.37550\n",
            "Epoch 1365: loss 0.37547\n",
            "Epoch 1366: loss 0.37545\n",
            "Epoch 1367: loss 0.37542\n",
            "Epoch 1368: loss 0.37539\n",
            "Epoch 1369: loss 0.37536\n",
            "Epoch 1370: loss 0.37534\n",
            "Epoch 1371: loss 0.37531\n",
            "Epoch 1372: loss 0.37528\n",
            "Epoch 1373: loss 0.37525\n",
            "Epoch 1374: loss 0.37523\n",
            "Epoch 1375: loss 0.37520\n",
            "Epoch 1376: loss 0.37517\n",
            "Epoch 1377: loss 0.37514\n",
            "Epoch 1378: loss 0.37512\n",
            "Epoch 1379: loss 0.37509\n",
            "Epoch 1380: loss 0.37506\n",
            "Epoch 1381: loss 0.37503\n",
            "Epoch 1382: loss 0.37501\n",
            "Epoch 1383: loss 0.37498\n",
            "Epoch 1384: loss 0.37495\n",
            "Epoch 1385: loss 0.37492\n",
            "Epoch 1386: loss 0.37490\n",
            "Epoch 1387: loss 0.37487\n",
            "Epoch 1388: loss 0.37484\n",
            "Epoch 1389: loss 0.37481\n",
            "Epoch 1390: loss 0.37479\n",
            "Epoch 1391: loss 0.37476\n",
            "Epoch 1392: loss 0.37473\n",
            "Epoch 1393: loss 0.37470\n",
            "Epoch 1394: loss 0.37468\n",
            "Epoch 1395: loss 0.37465\n",
            "Epoch 1396: loss 0.37462\n",
            "Epoch 1397: loss 0.37459\n",
            "Epoch 1398: loss 0.37457\n",
            "Epoch 1399: loss 0.37454\n",
            "Epoch 1400: loss 0.37451\n",
            "Epoch 1401: loss 0.37448\n",
            "Epoch 1402: loss 0.37446\n",
            "Epoch 1403: loss 0.37443\n",
            "Epoch 1404: loss 0.37440\n",
            "Epoch 1405: loss 0.37437\n",
            "Epoch 1406: loss 0.37434\n",
            "Epoch 1407: loss 0.37432\n",
            "Epoch 1408: loss 0.37429\n",
            "Epoch 1409: loss 0.37426\n",
            "Epoch 1410: loss 0.37423\n",
            "Epoch 1411: loss 0.37421\n",
            "Epoch 1412: loss 0.37418\n",
            "Epoch 1413: loss 0.37415\n",
            "Epoch 1414: loss 0.37412\n",
            "Epoch 1415: loss 0.37410\n",
            "Epoch 1416: loss 0.37407\n",
            "Epoch 1417: loss 0.37404\n",
            "Epoch 1418: loss 0.37401\n",
            "Epoch 1419: loss 0.37398\n",
            "Epoch 1420: loss 0.37396\n",
            "Epoch 1421: loss 0.37393\n",
            "Epoch 1422: loss 0.37390\n",
            "Epoch 1423: loss 0.37387\n",
            "Epoch 1424: loss 0.37384\n",
            "Epoch 1425: loss 0.37382\n",
            "Epoch 1426: loss 0.37379\n",
            "Epoch 1427: loss 0.37376\n",
            "Epoch 1428: loss 0.37373\n",
            "Epoch 1429: loss 0.37370\n",
            "Epoch 1430: loss 0.37368\n",
            "Epoch 1431: loss 0.37365\n",
            "Epoch 1432: loss 0.37362\n",
            "Epoch 1433: loss 0.37359\n",
            "Epoch 1434: loss 0.37356\n",
            "Epoch 1435: loss 0.37354\n",
            "Epoch 1436: loss 0.37351\n",
            "Epoch 1437: loss 0.37348\n",
            "Epoch 1438: loss 0.37345\n",
            "Epoch 1439: loss 0.37342\n",
            "Epoch 1440: loss 0.37339\n",
            "Epoch 1441: loss 0.37337\n",
            "Epoch 1442: loss 0.37334\n",
            "Epoch 1443: loss 0.37331\n",
            "Epoch 1444: loss 0.37328\n",
            "Epoch 1445: loss 0.37325\n",
            "Epoch 1446: loss 0.37322\n",
            "Epoch 1447: loss 0.37320\n",
            "Epoch 1448: loss 0.37317\n",
            "Epoch 1449: loss 0.37314\n",
            "Epoch 1450: loss 0.37311\n",
            "Epoch 1451: loss 0.37308\n",
            "Epoch 1452: loss 0.37305\n",
            "Epoch 1453: loss 0.37303\n",
            "Epoch 1454: loss 0.37300\n",
            "Epoch 1455: loss 0.37297\n",
            "Epoch 1456: loss 0.37294\n",
            "Epoch 1457: loss 0.37291\n",
            "Epoch 1458: loss 0.37288\n",
            "Epoch 1459: loss 0.37285\n",
            "Epoch 1460: loss 0.37282\n",
            "Epoch 1461: loss 0.37280\n",
            "Epoch 1462: loss 0.37277\n",
            "Epoch 1463: loss 0.37274\n",
            "Epoch 1464: loss 0.37271\n",
            "Epoch 1465: loss 0.37268\n",
            "Epoch 1466: loss 0.37265\n",
            "Epoch 1467: loss 0.37262\n",
            "Epoch 1468: loss 0.37259\n",
            "Epoch 1469: loss 0.37257\n",
            "Epoch 1470: loss 0.37254\n",
            "Epoch 1471: loss 0.37251\n",
            "Epoch 1472: loss 0.37248\n",
            "Epoch 1473: loss 0.37245\n",
            "Epoch 1474: loss 0.37242\n",
            "Epoch 1475: loss 0.37239\n",
            "Epoch 1476: loss 0.37236\n",
            "Epoch 1477: loss 0.37233\n",
            "Epoch 1478: loss 0.37230\n",
            "Epoch 1479: loss 0.37227\n",
            "Epoch 1480: loss 0.37224\n",
            "Epoch 1481: loss 0.37221\n",
            "Epoch 1482: loss 0.37219\n",
            "Epoch 1483: loss 0.37216\n",
            "Epoch 1484: loss 0.37213\n",
            "Epoch 1485: loss 0.37210\n",
            "Epoch 1486: loss 0.37207\n",
            "Epoch 1487: loss 0.37204\n",
            "Epoch 1488: loss 0.37201\n",
            "Epoch 1489: loss 0.37198\n",
            "Epoch 1490: loss 0.37195\n",
            "Epoch 1491: loss 0.37192\n",
            "Epoch 1492: loss 0.37189\n",
            "Epoch 1493: loss 0.37186\n",
            "Epoch 1494: loss 0.37183\n",
            "Epoch 1495: loss 0.37180\n",
            "Epoch 1496: loss 0.37177\n",
            "Epoch 1497: loss 0.37174\n",
            "Epoch 1498: loss 0.37171\n",
            "Epoch 1499: loss 0.37168\n",
            "Epoch 1500: loss 0.37165\n",
            "Epoch 1501: loss 0.37162\n",
            "Epoch 1502: loss 0.37159\n",
            "Epoch 1503: loss 0.37156\n",
            "Epoch 1504: loss 0.37153\n",
            "Epoch 1505: loss 0.37150\n",
            "Epoch 1506: loss 0.37147\n",
            "Epoch 1507: loss 0.37144\n",
            "Epoch 1508: loss 0.37141\n",
            "Epoch 1509: loss 0.37138\n",
            "Epoch 1510: loss 0.37135\n",
            "Epoch 1511: loss 0.37132\n",
            "Epoch 1512: loss 0.37129\n",
            "Epoch 1513: loss 0.37126\n",
            "Epoch 1514: loss 0.37123\n",
            "Epoch 1515: loss 0.37119\n",
            "Epoch 1516: loss 0.37116\n",
            "Epoch 1517: loss 0.37113\n",
            "Epoch 1518: loss 0.37110\n",
            "Epoch 1519: loss 0.37107\n",
            "Epoch 1520: loss 0.37104\n",
            "Epoch 1521: loss 0.37101\n",
            "Epoch 1522: loss 0.37098\n",
            "Epoch 1523: loss 0.37095\n",
            "Epoch 1524: loss 0.37092\n",
            "Epoch 1525: loss 0.37088\n",
            "Epoch 1526: loss 0.37085\n",
            "Epoch 1527: loss 0.37082\n",
            "Epoch 1528: loss 0.37079\n",
            "Epoch 1529: loss 0.37076\n",
            "Epoch 1530: loss 0.37073\n",
            "Epoch 1531: loss 0.37070\n",
            "Epoch 1532: loss 0.37066\n",
            "Epoch 1533: loss 0.37063\n",
            "Epoch 1534: loss 0.37060\n",
            "Epoch 1535: loss 0.37057\n",
            "Epoch 1536: loss 0.37054\n",
            "Epoch 1537: loss 0.37051\n",
            "Epoch 1538: loss 0.37047\n",
            "Epoch 1539: loss 0.37044\n",
            "Epoch 1540: loss 0.37041\n",
            "Epoch 1541: loss 0.37038\n",
            "Epoch 1542: loss 0.37034\n",
            "Epoch 1543: loss 0.37031\n",
            "Epoch 1544: loss 0.37028\n",
            "Epoch 1545: loss 0.37025\n",
            "Epoch 1546: loss 0.37022\n",
            "Epoch 1547: loss 0.37018\n",
            "Epoch 1548: loss 0.37015\n",
            "Epoch 1549: loss 0.37012\n",
            "Epoch 1550: loss 0.37008\n",
            "Epoch 1551: loss 0.37005\n",
            "Epoch 1552: loss 0.37002\n",
            "Epoch 1553: loss 0.36999\n",
            "Epoch 1554: loss 0.36995\n",
            "Epoch 1555: loss 0.36992\n",
            "Epoch 1556: loss 0.36989\n",
            "Epoch 1557: loss 0.36985\n",
            "Epoch 1558: loss 0.36982\n",
            "Epoch 1559: loss 0.36979\n",
            "Epoch 1560: loss 0.36975\n",
            "Epoch 1561: loss 0.36972\n",
            "Epoch 1562: loss 0.36969\n",
            "Epoch 1563: loss 0.36965\n",
            "Epoch 1564: loss 0.36962\n",
            "Epoch 1565: loss 0.36958\n",
            "Epoch 1566: loss 0.36955\n",
            "Epoch 1567: loss 0.36952\n",
            "Epoch 1568: loss 0.36948\n",
            "Epoch 1569: loss 0.36945\n",
            "Epoch 1570: loss 0.36941\n",
            "Epoch 1571: loss 0.36938\n",
            "Epoch 1572: loss 0.36935\n",
            "Epoch 1573: loss 0.36931\n",
            "Epoch 1574: loss 0.36928\n",
            "Epoch 1575: loss 0.36924\n",
            "Epoch 1576: loss 0.36921\n",
            "Epoch 1577: loss 0.36917\n",
            "Epoch 1578: loss 0.36914\n",
            "Epoch 1579: loss 0.36910\n",
            "Epoch 1580: loss 0.36907\n",
            "Epoch 1581: loss 0.36903\n",
            "Epoch 1582: loss 0.36900\n",
            "Epoch 1583: loss 0.36896\n",
            "Epoch 1584: loss 0.36893\n",
            "Epoch 1585: loss 0.36889\n",
            "Epoch 1586: loss 0.36886\n",
            "Epoch 1587: loss 0.36882\n",
            "Epoch 1588: loss 0.36879\n",
            "Epoch 1589: loss 0.36875\n",
            "Epoch 1590: loss 0.36872\n",
            "Epoch 1591: loss 0.36868\n",
            "Epoch 1592: loss 0.36865\n",
            "Epoch 1593: loss 0.36861\n",
            "Epoch 1594: loss 0.36858\n",
            "Epoch 1595: loss 0.36854\n",
            "Epoch 1596: loss 0.36851\n",
            "Epoch 1597: loss 0.36847\n",
            "Epoch 1598: loss 0.36843\n",
            "Epoch 1599: loss 0.36840\n",
            "Epoch 1600: loss 0.36836\n",
            "Epoch 1601: loss 0.36833\n",
            "Epoch 1602: loss 0.36829\n",
            "Epoch 1603: loss 0.36826\n",
            "Epoch 1604: loss 0.36822\n",
            "Epoch 1605: loss 0.36819\n",
            "Epoch 1606: loss 0.36815\n",
            "Epoch 1607: loss 0.36811\n",
            "Epoch 1608: loss 0.36808\n",
            "Epoch 1609: loss 0.36804\n",
            "Epoch 1610: loss 0.36801\n",
            "Epoch 1611: loss 0.36797\n",
            "Epoch 1612: loss 0.36794\n",
            "Epoch 1613: loss 0.36790\n",
            "Epoch 1614: loss 0.36787\n",
            "Epoch 1615: loss 0.36783\n",
            "Epoch 1616: loss 0.36780\n",
            "Epoch 1617: loss 0.36776\n",
            "Epoch 1618: loss 0.36773\n",
            "Epoch 1619: loss 0.36769\n",
            "Epoch 1620: loss 0.36766\n",
            "Epoch 1621: loss 0.36762\n",
            "Epoch 1622: loss 0.36759\n",
            "Epoch 1623: loss 0.36755\n",
            "Epoch 1624: loss 0.36752\n",
            "Epoch 1625: loss 0.36748\n",
            "Epoch 1626: loss 0.36745\n",
            "Epoch 1627: loss 0.36741\n",
            "Epoch 1628: loss 0.36738\n",
            "Epoch 1629: loss 0.36734\n",
            "Epoch 1630: loss 0.36731\n",
            "Epoch 1631: loss 0.36727\n",
            "Epoch 1632: loss 0.36724\n",
            "Epoch 1633: loss 0.36721\n",
            "Epoch 1634: loss 0.36717\n",
            "Epoch 1635: loss 0.36714\n",
            "Epoch 1636: loss 0.36711\n",
            "Epoch 1637: loss 0.36707\n",
            "Epoch 1638: loss 0.36704\n",
            "Epoch 1639: loss 0.36701\n",
            "Epoch 1640: loss 0.36697\n",
            "Epoch 1641: loss 0.36694\n",
            "Epoch 1642: loss 0.36691\n",
            "Epoch 1643: loss 0.36687\n",
            "Epoch 1644: loss 0.36684\n",
            "Epoch 1645: loss 0.36681\n",
            "Epoch 1646: loss 0.36678\n",
            "Epoch 1647: loss 0.36675\n",
            "Epoch 1648: loss 0.36671\n",
            "Epoch 1649: loss 0.36668\n",
            "Epoch 1650: loss 0.36665\n",
            "Epoch 1651: loss 0.36662\n",
            "Epoch 1652: loss 0.36659\n",
            "Epoch 1653: loss 0.36656\n",
            "Epoch 1654: loss 0.36652\n",
            "Epoch 1655: loss 0.36649\n",
            "Epoch 1656: loss 0.36646\n",
            "Epoch 1657: loss 0.36643\n",
            "Epoch 1658: loss 0.36640\n",
            "Epoch 1659: loss 0.36637\n",
            "Epoch 1660: loss 0.36634\n",
            "Epoch 1661: loss 0.36631\n",
            "Epoch 1662: loss 0.36628\n",
            "Epoch 1663: loss 0.36625\n",
            "Epoch 1664: loss 0.36622\n",
            "Epoch 1665: loss 0.36619\n",
            "Epoch 1666: loss 0.36616\n",
            "Epoch 1667: loss 0.36613\n",
            "Epoch 1668: loss 0.36610\n",
            "Epoch 1669: loss 0.36607\n",
            "Epoch 1670: loss 0.36604\n",
            "Epoch 1671: loss 0.36601\n",
            "Epoch 1672: loss 0.36598\n",
            "Epoch 1673: loss 0.36595\n",
            "Epoch 1674: loss 0.36592\n",
            "Epoch 1675: loss 0.36589\n",
            "Epoch 1676: loss 0.36586\n",
            "Epoch 1677: loss 0.36584\n",
            "Epoch 1678: loss 0.36581\n",
            "Epoch 1679: loss 0.36578\n",
            "Epoch 1680: loss 0.36575\n",
            "Epoch 1681: loss 0.36572\n",
            "Epoch 1682: loss 0.36569\n",
            "Epoch 1683: loss 0.36566\n",
            "Epoch 1684: loss 0.36563\n",
            "Epoch 1685: loss 0.36560\n",
            "Epoch 1686: loss 0.36557\n",
            "Epoch 1687: loss 0.36554\n",
            "Epoch 1688: loss 0.36551\n",
            "Epoch 1689: loss 0.36548\n",
            "Epoch 1690: loss 0.36545\n",
            "Epoch 1691: loss 0.36542\n",
            "Epoch 1692: loss 0.36539\n",
            "Epoch 1693: loss 0.36536\n",
            "Epoch 1694: loss 0.36533\n",
            "Epoch 1695: loss 0.36530\n",
            "Epoch 1696: loss 0.36527\n",
            "Epoch 1697: loss 0.36524\n",
            "Epoch 1698: loss 0.36521\n",
            "Epoch 1699: loss 0.36518\n",
            "Epoch 1700: loss 0.36515\n",
            "Epoch 1701: loss 0.36512\n",
            "Epoch 1702: loss 0.36509\n",
            "Epoch 1703: loss 0.36506\n",
            "Epoch 1704: loss 0.36503\n",
            "Epoch 1705: loss 0.36500\n",
            "Epoch 1706: loss 0.36496\n",
            "Epoch 1707: loss 0.36493\n",
            "Epoch 1708: loss 0.36490\n",
            "Epoch 1709: loss 0.36487\n",
            "Epoch 1710: loss 0.36484\n",
            "Epoch 1711: loss 0.36480\n",
            "Epoch 1712: loss 0.36477\n",
            "Epoch 1713: loss 0.36474\n",
            "Epoch 1714: loss 0.36470\n",
            "Epoch 1715: loss 0.36467\n",
            "Epoch 1716: loss 0.36464\n",
            "Epoch 1717: loss 0.36460\n",
            "Epoch 1718: loss 0.36457\n",
            "Epoch 1719: loss 0.36454\n",
            "Epoch 1720: loss 0.36450\n",
            "Epoch 1721: loss 0.36447\n",
            "Epoch 1722: loss 0.36443\n",
            "Epoch 1723: loss 0.36440\n",
            "Epoch 1724: loss 0.36436\n",
            "Epoch 1725: loss 0.36433\n",
            "Epoch 1726: loss 0.36429\n",
            "Epoch 1727: loss 0.36426\n",
            "Epoch 1728: loss 0.36422\n",
            "Epoch 1729: loss 0.36419\n",
            "Epoch 1730: loss 0.36415\n",
            "Epoch 1731: loss 0.36411\n",
            "Epoch 1732: loss 0.36408\n",
            "Epoch 1733: loss 0.36404\n",
            "Epoch 1734: loss 0.36400\n",
            "Epoch 1735: loss 0.36397\n",
            "Epoch 1736: loss 0.36393\n",
            "Epoch 1737: loss 0.36389\n",
            "Epoch 1738: loss 0.36386\n",
            "Epoch 1739: loss 0.36382\n",
            "Epoch 1740: loss 0.36378\n",
            "Epoch 1741: loss 0.36374\n",
            "Epoch 1742: loss 0.36371\n",
            "Epoch 1743: loss 0.36367\n",
            "Epoch 1744: loss 0.36363\n",
            "Epoch 1745: loss 0.36359\n",
            "Epoch 1746: loss 0.36356\n",
            "Epoch 1747: loss 0.36352\n",
            "Epoch 1748: loss 0.36348\n",
            "Epoch 1749: loss 0.36344\n",
            "Epoch 1750: loss 0.36340\n",
            "Epoch 1751: loss 0.36337\n",
            "Epoch 1752: loss 0.36333\n",
            "Epoch 1753: loss 0.36329\n",
            "Epoch 1754: loss 0.36325\n",
            "Epoch 1755: loss 0.36321\n",
            "Epoch 1756: loss 0.36317\n",
            "Epoch 1757: loss 0.36314\n",
            "Epoch 1758: loss 0.36310\n",
            "Epoch 1759: loss 0.36306\n",
            "Epoch 1760: loss 0.36302\n",
            "Epoch 1761: loss 0.36298\n",
            "Epoch 1762: loss 0.36295\n",
            "Epoch 1763: loss 0.36291\n",
            "Epoch 1764: loss 0.36287\n",
            "Epoch 1765: loss 0.36283\n",
            "Epoch 1766: loss 0.36279\n",
            "Epoch 1767: loss 0.36276\n",
            "Epoch 1768: loss 0.36272\n",
            "Epoch 1769: loss 0.36268\n",
            "Epoch 1770: loss 0.36264\n",
            "Epoch 1771: loss 0.36261\n",
            "Epoch 1772: loss 0.36257\n",
            "Epoch 1773: loss 0.36253\n",
            "Epoch 1774: loss 0.36249\n",
            "Epoch 1775: loss 0.36246\n",
            "Epoch 1776: loss 0.36242\n",
            "Epoch 1777: loss 0.36238\n",
            "Epoch 1778: loss 0.36235\n",
            "Epoch 1779: loss 0.36231\n",
            "Epoch 1780: loss 0.36227\n",
            "Epoch 1781: loss 0.36224\n",
            "Epoch 1782: loss 0.36220\n",
            "Epoch 1783: loss 0.36216\n",
            "Epoch 1784: loss 0.36213\n",
            "Epoch 1785: loss 0.36209\n",
            "Epoch 1786: loss 0.36206\n",
            "Epoch 1787: loss 0.36202\n",
            "Epoch 1788: loss 0.36198\n",
            "Epoch 1789: loss 0.36195\n",
            "Epoch 1790: loss 0.36191\n",
            "Epoch 1791: loss 0.36188\n",
            "Epoch 1792: loss 0.36184\n",
            "Epoch 1793: loss 0.36181\n",
            "Epoch 1794: loss 0.36177\n",
            "Epoch 1795: loss 0.36174\n",
            "Epoch 1796: loss 0.36171\n",
            "Epoch 1797: loss 0.36167\n",
            "Epoch 1798: loss 0.36164\n",
            "Epoch 1799: loss 0.36160\n",
            "Epoch 1800: loss 0.36157\n",
            "Epoch 1801: loss 0.36153\n",
            "Epoch 1802: loss 0.36150\n",
            "Epoch 1803: loss 0.36147\n",
            "Epoch 1804: loss 0.36143\n",
            "Epoch 1805: loss 0.36140\n",
            "Epoch 1806: loss 0.36137\n",
            "Epoch 1807: loss 0.36134\n",
            "Epoch 1808: loss 0.36130\n",
            "Epoch 1809: loss 0.36127\n",
            "Epoch 1810: loss 0.36124\n",
            "Epoch 1811: loss 0.36120\n",
            "Epoch 1812: loss 0.36117\n",
            "Epoch 1813: loss 0.36114\n",
            "Epoch 1814: loss 0.36111\n",
            "Epoch 1815: loss 0.36107\n",
            "Epoch 1816: loss 0.36104\n",
            "Epoch 1817: loss 0.36101\n",
            "Epoch 1818: loss 0.36098\n",
            "Epoch 1819: loss 0.36095\n",
            "Epoch 1820: loss 0.36091\n",
            "Epoch 1821: loss 0.36088\n",
            "Epoch 1822: loss 0.36085\n",
            "Epoch 1823: loss 0.36082\n",
            "Epoch 1824: loss 0.36079\n",
            "Epoch 1825: loss 0.36075\n",
            "Epoch 1826: loss 0.36072\n",
            "Epoch 1827: loss 0.36069\n",
            "Epoch 1828: loss 0.36066\n",
            "Epoch 1829: loss 0.36063\n",
            "Epoch 1830: loss 0.36059\n",
            "Epoch 1831: loss 0.36056\n",
            "Epoch 1832: loss 0.36053\n",
            "Epoch 1833: loss 0.36050\n",
            "Epoch 1834: loss 0.36047\n",
            "Epoch 1835: loss 0.36043\n",
            "Epoch 1836: loss 0.36040\n",
            "Epoch 1837: loss 0.36037\n",
            "Epoch 1838: loss 0.36034\n",
            "Epoch 1839: loss 0.36030\n",
            "Epoch 1840: loss 0.36027\n",
            "Epoch 1841: loss 0.36024\n",
            "Epoch 1842: loss 0.36021\n",
            "Epoch 1843: loss 0.36017\n",
            "Epoch 1844: loss 0.36014\n",
            "Epoch 1845: loss 0.36011\n",
            "Epoch 1846: loss 0.36007\n",
            "Epoch 1847: loss 0.36004\n",
            "Epoch 1848: loss 0.36000\n",
            "Epoch 1849: loss 0.35997\n",
            "Epoch 1850: loss 0.35994\n",
            "Epoch 1851: loss 0.35990\n",
            "Epoch 1852: loss 0.35987\n",
            "Epoch 1853: loss 0.35983\n",
            "Epoch 1854: loss 0.35979\n",
            "Epoch 1855: loss 0.35976\n",
            "Epoch 1856: loss 0.35972\n",
            "Epoch 1857: loss 0.35969\n",
            "Epoch 1858: loss 0.35965\n",
            "Epoch 1859: loss 0.35961\n",
            "Epoch 1860: loss 0.35957\n",
            "Epoch 1861: loss 0.35954\n",
            "Epoch 1862: loss 0.35950\n",
            "Epoch 1863: loss 0.35946\n",
            "Epoch 1864: loss 0.35942\n",
            "Epoch 1865: loss 0.35938\n",
            "Epoch 1866: loss 0.35934\n",
            "Epoch 1867: loss 0.35930\n",
            "Epoch 1868: loss 0.35926\n",
            "Epoch 1869: loss 0.35922\n",
            "Epoch 1870: loss 0.35918\n",
            "Epoch 1871: loss 0.35914\n",
            "Epoch 1872: loss 0.35909\n",
            "Epoch 1873: loss 0.35905\n",
            "Epoch 1874: loss 0.35901\n",
            "Epoch 1875: loss 0.35896\n",
            "Epoch 1876: loss 0.35892\n",
            "Epoch 1877: loss 0.35887\n",
            "Epoch 1878: loss 0.35882\n",
            "Epoch 1879: loss 0.35878\n",
            "Epoch 1880: loss 0.35873\n",
            "Epoch 1881: loss 0.35868\n",
            "Epoch 1882: loss 0.35863\n",
            "Epoch 1883: loss 0.35858\n",
            "Epoch 1884: loss 0.35853\n",
            "Epoch 1885: loss 0.35848\n",
            "Epoch 1886: loss 0.35843\n",
            "Epoch 1887: loss 0.35838\n",
            "Epoch 1888: loss 0.35832\n",
            "Epoch 1889: loss 0.35827\n",
            "Epoch 1890: loss 0.35822\n",
            "Epoch 1891: loss 0.35816\n",
            "Epoch 1892: loss 0.35810\n",
            "Epoch 1893: loss 0.35804\n",
            "Epoch 1894: loss 0.35799\n",
            "Epoch 1895: loss 0.35793\n",
            "Epoch 1896: loss 0.35786\n",
            "Epoch 1897: loss 0.35780\n",
            "Epoch 1898: loss 0.35774\n",
            "Epoch 1899: loss 0.35767\n",
            "Epoch 1900: loss 0.35761\n",
            "Epoch 1901: loss 0.35754\n",
            "Epoch 1902: loss 0.35747\n",
            "Epoch 1903: loss 0.35740\n",
            "Epoch 1904: loss 0.35733\n",
            "Epoch 1905: loss 0.35726\n",
            "Epoch 1906: loss 0.35718\n",
            "Epoch 1907: loss 0.35710\n",
            "Epoch 1908: loss 0.35702\n",
            "Epoch 1909: loss 0.35694\n",
            "Epoch 1910: loss 0.35686\n",
            "Epoch 1911: loss 0.35677\n",
            "Epoch 1912: loss 0.35668\n",
            "Epoch 1913: loss 0.35659\n",
            "Epoch 1914: loss 0.35649\n",
            "Epoch 1915: loss 0.35640\n",
            "Epoch 1916: loss 0.35630\n",
            "Epoch 1917: loss 0.35619\n",
            "Epoch 1918: loss 0.35608\n",
            "Epoch 1919: loss 0.35597\n",
            "Epoch 1920: loss 0.35586\n",
            "Epoch 1921: loss 0.35574\n",
            "Epoch 1922: loss 0.35562\n",
            "Epoch 1923: loss 0.35550\n",
            "Epoch 1924: loss 0.35537\n",
            "Epoch 1925: loss 0.35524\n",
            "Epoch 1926: loss 0.35511\n",
            "Epoch 1927: loss 0.35497\n",
            "Epoch 1928: loss 0.35484\n",
            "Epoch 1929: loss 0.35470\n",
            "Epoch 1930: loss 0.35456\n",
            "Epoch 1931: loss 0.35442\n",
            "Epoch 1932: loss 0.35428\n",
            "Epoch 1933: loss 0.35414\n",
            "Epoch 1934: loss 0.35401\n",
            "Epoch 1935: loss 0.35387\n",
            "Epoch 1936: loss 0.35374\n",
            "Epoch 1937: loss 0.35361\n",
            "Epoch 1938: loss 0.35348\n",
            "Epoch 1939: loss 0.35336\n",
            "Epoch 1940: loss 0.35324\n",
            "Epoch 1941: loss 0.35312\n",
            "Epoch 1942: loss 0.35301\n",
            "Epoch 1943: loss 0.35290\n",
            "Epoch 1944: loss 0.35279\n",
            "Epoch 1945: loss 0.35269\n",
            "Epoch 1946: loss 0.35259\n",
            "Epoch 1947: loss 0.35249\n",
            "Epoch 1948: loss 0.35240\n",
            "Epoch 1949: loss 0.35231\n",
            "Epoch 1950: loss 0.35222\n",
            "Epoch 1951: loss 0.35214\n",
            "Epoch 1952: loss 0.35205\n",
            "Epoch 1953: loss 0.35197\n",
            "Epoch 1954: loss 0.35189\n",
            "Epoch 1955: loss 0.35182\n",
            "Epoch 1956: loss 0.35174\n",
            "Epoch 1957: loss 0.35167\n",
            "Epoch 1958: loss 0.35160\n",
            "Epoch 1959: loss 0.35152\n",
            "Epoch 1960: loss 0.35146\n",
            "Epoch 1961: loss 0.35139\n",
            "Epoch 1962: loss 0.35132\n",
            "Epoch 1963: loss 0.35125\n",
            "Epoch 1964: loss 0.35119\n",
            "Epoch 1965: loss 0.35113\n",
            "Epoch 1966: loss 0.35106\n",
            "Epoch 1967: loss 0.35100\n",
            "Epoch 1968: loss 0.35094\n",
            "Epoch 1969: loss 0.35088\n",
            "Epoch 1970: loss 0.35082\n",
            "Epoch 1971: loss 0.35076\n",
            "Epoch 1972: loss 0.35071\n",
            "Epoch 1973: loss 0.35065\n",
            "Epoch 1974: loss 0.35059\n",
            "Epoch 1975: loss 0.35053\n",
            "Epoch 1976: loss 0.35048\n",
            "Epoch 1977: loss 0.35042\n",
            "Epoch 1978: loss 0.35037\n",
            "Epoch 1979: loss 0.35031\n",
            "Epoch 1980: loss 0.35026\n",
            "Epoch 1981: loss 0.35021\n",
            "Epoch 1982: loss 0.35015\n",
            "Epoch 1983: loss 0.35010\n",
            "Epoch 1984: loss 0.35005\n",
            "Epoch 1985: loss 0.34999\n",
            "Epoch 1986: loss 0.34994\n",
            "Epoch 1987: loss 0.34989\n",
            "Epoch 1988: loss 0.34984\n",
            "Epoch 1989: loss 0.34979\n",
            "Epoch 1990: loss 0.34973\n",
            "Epoch 1991: loss 0.34968\n",
            "Epoch 1992: loss 0.34963\n",
            "Epoch 1993: loss 0.34958\n",
            "Epoch 1994: loss 0.34953\n",
            "Epoch 1995: loss 0.34948\n",
            "Epoch 1996: loss 0.34943\n",
            "Epoch 1997: loss 0.34938\n",
            "Epoch 1998: loss 0.34933\n",
            "Epoch 1999: loss 0.34927\n",
            "Epoch 2000: loss 0.34922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyTjLzELSdQF"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZafmlssFs5SG",
        "outputId": "72f2896a-39dc-4b9d-c5e6-998efc759690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "network.eval()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=8, out_features=5, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (3): Sigmoid()\n",
              "  (4): Linear(in_features=5, out_features=1, bias=True)\n",
              "  (5): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuL49MUEtOrh"
      },
      "source": [
        "X_test = torch.tensor(X_test, dtype=torch.float)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe8muTxbtc1G"
      },
      "source": [
        "predictions = network.forward(X_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn0leHiAtqpx",
        "outputId": "dd4fee3b-1106-4adc-fca0-f9bd68759bff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.9972e-01],\n",
              "        [2.3615e-01],\n",
              "        [2.4651e-01],\n",
              "        [9.9780e-01],\n",
              "        [9.3357e-02],\n",
              "        [5.6443e-03],\n",
              "        [1.9993e-01],\n",
              "        [9.9984e-01],\n",
              "        [4.7229e-01],\n",
              "        [5.3334e-04],\n",
              "        [9.0116e-03],\n",
              "        [5.1698e-01],\n",
              "        [5.0906e-01],\n",
              "        [7.2133e-04],\n",
              "        [1.6928e-01],\n",
              "        [5.4408e-05],\n",
              "        [5.5646e-02],\n",
              "        [1.8075e-08],\n",
              "        [4.0451e-01],\n",
              "        [5.6221e-01],\n",
              "        [3.1149e-01],\n",
              "        [2.8012e-01],\n",
              "        [9.9977e-01],\n",
              "        [9.7627e-01],\n",
              "        [9.6177e-05],\n",
              "        [2.4846e-01],\n",
              "        [1.0700e-01],\n",
              "        [1.0000e+00],\n",
              "        [2.1483e-01],\n",
              "        [6.1153e-05],\n",
              "        [9.9767e-01],\n",
              "        [8.2462e-01],\n",
              "        [1.7584e-01],\n",
              "        [2.1306e-01],\n",
              "        [1.1638e-01],\n",
              "        [1.0000e+00],\n",
              "        [9.9528e-01],\n",
              "        [9.9995e-01],\n",
              "        [1.9430e-03],\n",
              "        [1.0000e+00],\n",
              "        [2.0546e-03],\n",
              "        [2.4281e-05],\n",
              "        [5.8140e-02],\n",
              "        [6.4338e-01],\n",
              "        [1.2243e-04],\n",
              "        [2.9091e-02],\n",
              "        [9.5467e-02],\n",
              "        [2.9317e-01],\n",
              "        [9.5075e-02],\n",
              "        [2.5084e-01],\n",
              "        [1.8234e-05],\n",
              "        [6.1497e-06],\n",
              "        [7.6644e-02],\n",
              "        [1.8148e-01],\n",
              "        [9.8952e-01],\n",
              "        [7.5150e-02],\n",
              "        [9.3812e-02],\n",
              "        [5.8185e-05],\n",
              "        [1.7007e-04],\n",
              "        [6.8956e-01],\n",
              "        [2.4236e-01],\n",
              "        [2.6312e-01],\n",
              "        [1.3374e-01],\n",
              "        [9.2057e-01],\n",
              "        [2.2463e-01],\n",
              "        [2.8393e-08],\n",
              "        [9.6489e-01],\n",
              "        [6.6666e-01],\n",
              "        [9.4653e-02],\n",
              "        [3.2976e-04],\n",
              "        [4.4832e-05],\n",
              "        [2.2297e-05],\n",
              "        [3.6979e-01],\n",
              "        [1.4581e-05],\n",
              "        [6.7000e-01],\n",
              "        [1.3187e-02],\n",
              "        [3.3789e-02],\n",
              "        [4.1876e-02],\n",
              "        [1.0363e-02],\n",
              "        [3.3001e-01],\n",
              "        [1.6256e-01],\n",
              "        [1.7366e-01],\n",
              "        [6.2832e-01],\n",
              "        [9.9194e-01],\n",
              "        [7.3266e-05],\n",
              "        [2.9275e-01],\n",
              "        [1.7731e-04],\n",
              "        [1.5849e-01],\n",
              "        [5.2533e-01],\n",
              "        [1.8456e-02],\n",
              "        [4.3262e-01],\n",
              "        [5.5761e-01],\n",
              "        [1.2470e-04],\n",
              "        [1.0044e-01],\n",
              "        [2.6482e-01],\n",
              "        [6.6297e-01],\n",
              "        [1.2788e-04],\n",
              "        [1.2947e-04],\n",
              "        [6.8128e-02],\n",
              "        [6.9197e-01],\n",
              "        [6.7519e-03],\n",
              "        [1.7274e-02],\n",
              "        [1.1765e-03],\n",
              "        [7.6582e-01],\n",
              "        [6.7821e-01],\n",
              "        [5.8446e-01],\n",
              "        [1.0881e-01],\n",
              "        [7.9792e-01],\n",
              "        [9.7251e-01],\n",
              "        [1.1288e-04],\n",
              "        [1.9341e-02],\n",
              "        [7.7012e-05],\n",
              "        [3.7499e-02],\n",
              "        [6.9195e-04],\n",
              "        [9.3548e-05],\n",
              "        [5.0294e-01],\n",
              "        [1.3419e-04],\n",
              "        [7.0711e-02],\n",
              "        [6.5849e-01],\n",
              "        [2.5435e-03],\n",
              "        [3.2204e-01],\n",
              "        [2.5980e-01],\n",
              "        [7.0704e-02],\n",
              "        [1.0167e-07],\n",
              "        [7.3390e-02],\n",
              "        [1.3635e-04],\n",
              "        [1.1619e-04],\n",
              "        [1.4792e-01],\n",
              "        [9.9990e-01],\n",
              "        [9.9998e-01],\n",
              "        [7.4008e-02],\n",
              "        [3.7833e-01],\n",
              "        [9.4597e-01],\n",
              "        [6.4964e-05],\n",
              "        [7.1144e-02],\n",
              "        [8.4613e-02],\n",
              "        [3.0913e-01],\n",
              "        [8.7922e-02],\n",
              "        [3.0310e-01],\n",
              "        [4.0956e-01],\n",
              "        [9.2474e-02],\n",
              "        [3.9394e-01],\n",
              "        [6.1878e-01],\n",
              "        [9.4917e-01],\n",
              "        [4.1019e-01],\n",
              "        [1.5014e-05],\n",
              "        [9.8460e-01],\n",
              "        [5.9925e-02],\n",
              "        [6.2424e-01],\n",
              "        [1.0072e-02],\n",
              "        [8.5574e-01],\n",
              "        [9.8647e-01],\n",
              "        [1.0969e-01],\n",
              "        [9.5966e-01]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu8CV7XawV_K",
        "outputId": "4ce7ebb8-7db5-4aeb-dccb-e895eb29f50b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions = (predictions >= 0.5)\n",
        "predictions"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [ True]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVPFTbGeuSU5",
        "outputId": "977148b4-7776-4d7e-ba65-b2928b7b3db8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_test"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
              "       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7jfnFtZuNVn",
        "outputId": "f41fe50c-ff3c-47b1-884b-07b3bc0a70fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "accuracy_score(y_test, predictions.detach().numpy())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7792207792207793"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYLq3Y7ctnbS",
        "outputId": "4f701d7b-8f5f-4cb0-af19-6cf45d17b157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cm = confusion_matrix(y_test, predictions.detach().numpy())\n",
        "cm"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[90, 15],\n",
              "       [19, 30]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3C7dimytqG6",
        "outputId": "2495edaf-d8c6-4219-c10c-d9d61b65e6bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb1879d9990>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion: after running our model we have received 78 % accuracy and can see that 90 were classified correctly as class 0 and 15 wrong as class 1 and 30 correct as class 1 and 19 wrong as class 0. **"
      ],
      "metadata": {
        "id": "7wzaIWxg4tIt"
      }
    }
  ]
}